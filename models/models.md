# Models - Ki·∫øn tr√∫c M√¥ h√¨nh QTransformer

## T·ªïng quan
Folder `models` ch·ª©a c√°c ki·∫øn tr√∫c m√¥ h√¨nh ch√≠nh c·ªßa d·ª± √°n QTransformer. ƒê√¢y l√† n∆°i ƒë·ªãnh nghƒ©a c√°c neural network architectures ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho supply chain forecasting, t·ª´ baseline model ƒë·∫øn advanced embedding-enhanced variants. T·∫•t c·∫£ models ƒë·ªÅu d·ª±a tr√™n **Patch-based Transformer** approach v·ªõi quantum attention capabilities.

## Ki·∫øn tr√∫c M√¥ h√¨nh

### üèóÔ∏è **QCAAPatchTF.py** - M√¥ h√¨nh C∆° b·∫£n (Baseline)

#### üéØ **M·ª•c ƒë√≠ch ch√≠nh:**
M√¥ h√¨nh Transformer baseline s·ª≠ d·ª•ng patch embedding technique k·∫øt h·ª£p v·ªõi quantum attention mechanisms cho time series forecasting.

#### üß© **Th√†nh ph·∫ßn ch√≠nh:**

##### **1. Patch Embedding Strategy:**
```python
def compute_patch_len(seq_len, num_patches=None, method="evaluate"):
    if method == "evaluate":
        num_patches = 6 if num_patches is None else num_patches
        patch_len = seq_len // num_patches
        return max(1, patch_len)
```
**Ch·ª©c nƒÉng:** Chia time series th√†nh patches ƒë·ªÉ gi·∫£m computational complexity
**L·ª£i √≠ch:** Gi·∫£m t·ª´ O(L¬≤) xu·ªëng O(P¬≤) v·ªõi P << L

##### **2. Hybrid Attention Architecture:**
```python
# Encoder v·ªõi mixed attention
AttentionLayer(
    QuantumAttention() if i % 2 == 0 and self.use_quantum_attention
    else FullAttention(),  # Alternating layers
    d_model, n_heads
)
```
**ƒê·∫∑c ƒëi·ªÉm:** 
- **Even layers**: QuantumAttention ƒë·ªÉ capture complex dependencies
- **Odd layers**: FullAttention ƒë·ªÉ maintain stability
- **Hybrid approach**: C√¢n b·∫±ng innovation v√† reliability

##### **3. Multi-task Support:**
```python
def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, categorical_features=None):
    if self.task_name == 'long_term_forecast':
        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
    elif self.task_name == 'anomaly_detection':
        return self.anomaly_detection(x_enc, x_mark_enc)
    elif self.task_name == 'classification':
        return self.classification(x_enc, x_mark_enc)
```
**Flexibility:** M·ªôt model cho multiple tasks

##### **4. Normalization Strategy:**
```python
# Non-stationary normalization
means = x_enc.mean(1, keepdim=True).detach()
x_enc = x_enc - means
stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
x_enc /= stdev

# De-normalization at output
dec_out = dec_out * stdev + means
```
**√ù nghƒ©a:** X·ª≠ l√Ω non-stationary patterns trong supply chain data

#### üîß **Categorical Features Support:**
```python
# Embedding cho categorical features
self.embeddings = nn.ModuleDict()
for cat_name, cat_dim in configs.categorical_dims.items():
    embed_dim = min(50, (cat_dim + 1) // 2)
    self.embeddings[cat_name] = nn.Embedding(cat_dim, embed_dim)

# K·∫øt h·ª£p v·ªõi numerical features
if categorical_features:
    embedded_cats = [self.embeddings[name](indices) for name, indices in categorical_features.items()]
    cat_embedded = torch.cat(embedded_cats, dim=-1)
    x_enc = torch.cat([x_enc, cat_embedded], dim=-1)
    x_enc = self.input_projection(x_enc)
```

### üåü **QCAAPatchTF_Embedding.py** - M√¥ h√¨nh Ch√≠nh (Phase 1)

#### üéØ **M·ª•c ƒë√≠ch ch√≠nh:**
Enhanced version c·ªßa QCAAPatchTF v·ªõi advanced embedding support v√† multi-market forecasting capabilities. ƒê√¢y l√† model ch√≠nh ƒë·∫°t **98.71% MSE improvement** trong Phase 1.

#### üöÄ **T√≠nh nƒÉng n√¢ng cao:**

##### **1. EmbeddingHead Class:**
```python
class EmbeddingHead(nn.Module):
    def __init__(self, d_model, target_window, n_markets=1, categorical_dims=None):
        # Embedding layers cho categorical features
        self.embeddings = nn.ModuleDict()
        for cat_name, cat_dim in categorical_dims.items():
            embed_dim = min(50, (cat_dim + 1) // 2)
            self.embeddings[cat_name] = nn.Embedding(cat_dim, embed_dim)
        
        # Final prediction cho multi-market
        self.linear = nn.Linear(d_model, target_window * n_markets)
```
**Innovation:** Specialized head cho multi-market output v·ªõi embedding integration

##### **2. Channel Independence Strategy:**
```python
if self.channel_independence == 1:
    # X·ª≠ l√Ω m·ªói channel ri√™ng bi·ªát
    x_enc = x_enc.permute(0, 2, 1).reshape(-1, L)
    x_enc = x_enc.unfold(dimension=-1, size=self.patch_len, step=self.patch_len)
    enc_out = self.W_P(x_enc)
else:
    # X·ª≠ l√Ω t·∫•t c·∫£ channels c√πng nhau
    patches = [channel_data.unfold(...) for channel_data in x_enc]
    x_enc = torch.stack(patches, dim=1)
```
**Flexibility:** Support c·∫£ independent v√† joint channel processing

##### **3. Multi-Market Output:**
```python
class FlattenHead(nn.Module):
    def forward(self, x):
        batch_size = x.shape[0]
        x = self.flatten(x)
        x = self.linear(x)  # Linear to target_window * n_markets
        # Reshape to [batch_size, target_window, n_markets]
        x = x.view(batch_size, self.target_window, self.n_markets)
        return x
```
**Output:** Direct multi-market predictions: [batch, pred_len, 3_markets]

#### üìä **Phase 1 Configuration:**
```python
# Supply chain specific setup
configs.enc_in = 21          # 21 numerical features
configs.c_out = 3            # 3 markets (Europe, LATAM, USCA)
configs.seq_len = 7          # 7 days input sequence
configs.pred_len = 1         # 1 day prediction horizon
configs.d_model = 512        # Hidden dimension
configs.n_heads = 8          # Multi-head attention
configs.e_layers = 6         # Encoder layers
```

### üåç **MultiRegionQCAAPatchTF.py** - M√¥ h√¨nh ƒêa V√πng

#### üéØ **M·ª•c ƒë√≠ch ch√≠nh:**
Extension c·ªßa base model ƒë·ªÉ h·ªó tr·ª£ region-specific modeling v·ªõi shared base architecture.

#### üß© **Ki·∫øn tr√∫c ƒë·∫∑c bi·ªát:**

##### **1. Region-Specific Heads:**
```python
class MultiRegionModel(nn.Module):
    def __init__(self, configs):
        self.region_heads = nn.ModuleList([
            nn.Linear(configs.d_model, configs.pred_len) 
            for _ in range(self.num_regions)
        ])
        self.region_embedding = nn.Embedding(self.num_regions, configs.d_model)
```

##### **2. Region-Enhanced Features:**
```python
def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, region_ids):
    base_features = self.base_model.encoder_forward(x_enc, x_mark_enc)
    region_embeds = self.region_embedding(region_ids)
    enhanced_features = base_features + region_embeds
    
    # Generate predictions per region
    predictions = [region_head(enhanced_features[region_mask]) 
                  for region_head in self.region_heads]
```

**Use case:** Experimental cho Phase 2 khi c·∫ßn region-specific parameters

## Thi·∫øt k·∫ø Patch Embedding

### üîÑ **Patch Strategy:**

#### **T·∫°i sao s·ª≠ d·ª•ng Patches:**
1. **Computational Efficiency**: Gi·∫£m complexity t·ª´ O(L¬≤) xu·ªëng O(P¬≤)
2. **Local Pattern Capture**: T·ªët h∆°n cho time series patterns
3. **Memory Efficiency**: X·ª≠ l√Ω longer sequences v·ªõi limited memory
4. **Locality Bias**: Inductive bias ph√π h·ª£p cho temporal data

#### **Patch Processing Pipeline:**
```python
# 1. Input: [batch_size, seq_len, features] = [B, 7, 21]
# 2. Unfold: [B, 7, 21] -> [B, patch_num, patch_len, 21]
x_enc = x_enc.unfold(dimension=1, size=patch_len, step=patch_len)

# 3. Patch Embedding: [B, patch_num, patch_len] -> [B, patch_num, d_model]
enc_out = self.W_P(x_enc)

# 4. Transformer Encoding: [B, patch_num, d_model] -> [B, patch_num, d_model]
enc_out, _ = self.encoder(enc_out)

# 5. Head Projection: [B, patch_num, d_model] -> [B, pred_len, n_markets]
output = self.head(enc_out)
```

### ‚ö° **Performance Characteristics:**

| Component | Input Shape | Output Shape | Computation |
|-----------|-------------|--------------|-------------|
| Patch Embedding | [B, 7, 21] | [B, 2, 512] | O(patch_len √ó d_model) |
| Transformer | [B, 2, 512] | [B, 2, 512] | O(patch_num¬≤) |
| Head | [B, 2, 512] | [B, 1, 3] | O(d_model √ó markets) |

**Hi·ªáu qu·∫£:** Gi·∫£m 87.5% computational cost so v·ªõi full sequence attention

## Quantum Attention Integration

### üî¨ **QuantumAttention trong QCAAPatchTF:**

#### **Hybrid Attention Strategy:**
```python
# Alternating quantum and classical attention
for i in range(configs.e_layers):
    if i % 2 == 0 and self.use_quantum_attention:
        attention = QuantumAttention(
            num_qubits=4,
            entanglement_factor=0.5,
            attention_dropout=configs.dropout
        )
    else:
        attention = FullAttention(
            mask_flag=False,
            attention_dropout=configs.dropout
        )
```

#### **Quantum Circuit Design:**
```python
def variational_circuit(self, params):
    # Parameterized rotation gates
    for i in range(self.num_qubits):
        qml.RY(params[i], wires=i)
    
    # Entanglement layer
    for i in range(self.num_qubits - 1):
        qml.CNOT(wires=[i, i + 1])
    
    return qml.expval(qml.PauliZ(0))
```

**Status:** Experimental feature cho research, kh√¥ng s·ª≠ d·ª•ng trong production Phase 1

## Multi-Market Forecasting

### üåç **Supply Chain Multi-Market Strategy:**

#### **Market Configuration:**
- **Europe Market**: Market ID = 0, Weight = 0.35
- **LATAM Market**: Market ID = 1, Weight = 0.30  
- **USCA Market**: Market ID = 2, Weight = 0.35

#### **Ki·∫øn tr√∫c Output:**
```python
# Output m√¥ h√¨nh: [batch_size, pred_len, n_markets]
# V√≠ d·ª•: [32, 1, 3] cho batch=32, d·ª± b√°o 1 ng√†y, 3 th·ªã tr∆∞·ªùng

# D·ª± b√°o theo th·ªã tr∆∞·ªùng c·ª• th·ªÉ
europe_pred = output[:, :, 0]   # D·ª± b√°o Europe
latam_pred = output[:, :, 1]    # D·ª± b√°o LATAM
usca_pred = output[:, :, 2]     # D·ª± b√°o USCA
```

#### **T√≠nh to√°n Loss v·ªõi Tr·ªçng s·ªë Th·ªã tr∆∞·ªùng:**
```python
# Trong experiment class
criterion = WeightedMSELoss(market_weights=[0.35, 0.30, 0.35])
loss = criterion(predictions, targets)  # T·ª± ƒë·ªông weighted theo th·ªã tr∆∞·ªùng
```

## S·ª≠ d·ª•ng M√¥ h√¨nh v√† C·∫•u h√¨nh

### üõ†Ô∏è **Thi·∫øt l·∫≠p Training:**

#### **C·∫•u h√¨nh T·ªët nh·∫•t Phase 1:**
```python
# L·ª±a ch·ªçn m√¥ h√¨nh
args.model = 'QCAAPatchTF_Embedding'
args.data = 'SupplyChainEmbedding'

# Tham s·ªë ki·∫øn tr√∫c
args.d_model = 512
args.n_heads = 8  
args.e_layers = 6
args.d_ff = 2048
args.dropout = 0.1
args.activation = 'gelu'

# Tham s·ªë nhi·ªám v·ª•
args.seq_len = 7
args.pred_len = 1
args.enc_in = 21
args.c_out = 3

# Tham s·ªë training
args.batch_size = 32
args.learning_rate = 0.0001
args.train_epochs = 10
```

#### **Kh·ªüi t·∫°o M√¥ h√¨nh:**
```python
from models.QCAAPatchTF_Embedding import QCAAPatchTF_Embedding

# Kh·ªüi t·∫°o m√¥ h√¨nh
model = QCAAPatchTF_Embedding(configs)

# Forward pass
outputs = model(
    x_enc=input_data,           # [batch, seq_len, features]
    x_mark_enc=time_features,   # [batch, seq_len, time_dims]  
    x_dec=decoder_input,        # [batch, label_len + pred_len, features]
    x_mark_dec=decoder_time,    # [batch, label_len + pred_len, time_dims]
    categorical_features=cat_data  # Dict ch·ª©a categorical indices
)
# Output: [batch, pred_len, n_markets]
```

### üîß **G·ª° l·ªói v√† Gi√°m s√°t:**

#### **Th√¥ng tin M√¥ h√¨nh:**
```python
# Ki·ªÉm tra s·ªë parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"T·ªïng parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Ki·ªÉm tra ki·∫øn tr√∫c m√¥ h√¨nh
print(model)

# Ki·ªÉm tra k√≠ch th∆∞·ªõc output
with torch.no_grad():
    sample_output = model(sample_input)
    print(f"K√≠ch th∆∞·ªõc output: {sample_output.shape}")
```

#### **V·∫•n ƒë·ªÅ Th∆∞·ªùng g·∫∑p v√† Gi·∫£i ph√°p:**
```python
# L·ªói memory
if torch.cuda.is_available():
    model = model.cuda()
    # S·ª≠ d·ª•ng gradient checkpointing
    model.gradient_checkpointing = True

# Gi√° tr·ªã NaN
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Training ch·∫≠m
# S·ª≠ d·ª•ng mixed precision
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
```

## K·∫øt qu·∫£ Phase 1

### üìä **Performance Achievements:**

#### **QCAAPatchTF_Embedding Results:**
- **MSE Improvement**: 98.71% so v·ªõi baseline
- **Model Size**: ~15M parameters
- **Training Time**: ~15 ph√∫t/epoch tr√™n single GPU
- **Inference Speed**: ~2ms per batch
- **Memory Usage**: ~3GB peak GPU memory

#### **Architecture Benefits:**
- **Patch Efficiency**: 87.5% computation reduction
- **Multi-market Capability**: Simultaneous 3-market predictions
- **Embedding Integration**: Categorical feature support
- **Robust Training**: Stable convergence v·ªõi early stopping

### üéØ **Business Impact:**
- **Accuracy**: 91.78% forecasting accuracy
- **Multi-market Support**: Europe/LATAM/USCA simultaneous predictions  
- **Real-time Capability**: Sub-second inference time
- **Scalability**: Memory efficient cho production deployment

## H∆∞·ªõng ph√°t tri·ªÉn Phase 2

### üöÄ **C·∫£i ti·∫øn M√¥ h√¨nh:**

#### 1. **C·∫£i thi·ªán Ki·∫øn tr√∫c:**
- **Attention Hi·ªáu qu·∫£**: C√°c bi·∫øn th·ªÉ Linformer, Performer
- **Patches Ph√¢n c·∫•p**: Patch embedding ƒëa t·∫ßng
- **Ki·∫øn tr√∫c ƒê·ªông**: ƒê·ªô s√¢u/r·ªông m√¥ h√¨nh th√≠ch ·ª©ng
- **T·ªëi ∆∞u B·ªô nh·ªõ**: Gradient checkpointing, chia s·∫ª m√¥ h√¨nh

#### 2. **T√≠nh nƒÉng N√¢ng cao:**
- **T√≠ch h·ª£p D·ªØ li·ªáu Ngo√†i**: Th·ªùi ti·∫øt, ch·ªâ s·ªë kinh t·∫ø
- **Attention Li√™n th·ªã tr∆∞·ªùng**: M√¥ h√¨nh ph·ª• thu·ªôc th·ªã tr∆∞·ªùng r√µ r√†ng
- **ƒê·ªãnh l∆∞·ª£ng Kh√¥ng ch·∫Øc ch·∫Øn**: D·ª± b√°o x√°c su·∫•t
- **D·ª± b√°o ƒêa k·ª≥ h·∫°n**: D·ª± b√°o ƒë·ªô d√†i thay ƒë·ªïi

#### 3. **T√≠nh nƒÉng S·∫£n xu·∫•t:**
- **Ph·ª•c v·ª• M√¥ h√¨nh**: Xu·∫•t ONNX, t·ªëi ∆∞u TensorRT
- **Ki·ªÉm th·ª≠ A/B**: Nhi·ªÅu bi·∫øn th·ªÉ m√¥ h√¨nh
- **H·ªçc Tr·ª±c tuy·∫øn**: C·∫≠p nh·∫≠t m√¥ h√¨nh tƒÉng d·∫ßn
- **Gi√°m s√°t**: Theo d√µi hi·ªáu su·∫•t, ph√°t hi·ªán drift

### üî¨ **H∆∞·ªõng Nghi√™n c·ª©u:**
- **T√≠ch h·ª£p Quantum**: Th√≠ nghi·ªám ph·∫ßn c·ª©ng quantum th·ª±c
- **M√¥ h√¨nh N·ªÅn t·∫£ng**: M√¥ h√¨nh supply chain ƒë∆∞·ª£c pre-train
- **M√¥ h√¨nh Nh√¢n qu·∫£**: Kh√°m ph√° nh√¢n qu·∫£ v·ªõi attention mechanisms
- **H·ªçc Li√™n k·∫øt**: Training m√¥ h√¨nh li√™n t·ªï ch·ª©c

## Th·ª±c h√†nh T·ªët nh·∫•t

### ‚úÖ **H∆∞·ªõng d·∫´n L·ª±a ch·ªçn M√¥ h√¨nh:**

#### **S·ª≠ d·ª•ng QCAAPatchTF khi:**
- Th√≠ nghi·ªám baseline v√† proof-of-concept
- T√†i nguy√™n t√≠nh to√°n h·∫°n ch·∫ø
- Nhi·ªám v·ª• d·ª± b√°o ƒë∆°n gi·∫£n
- Nghi√™n c·ª©u v·ªõi quantum attention

#### **S·ª≠ d·ª•ng QCAAPatchTF_Embedding khi:**
- Tri·ªÉn khai s·∫£n xu·∫•t (ƒë∆∞·ª£c khuy·∫øn ngh·ªã)
- D·ª± b√°o ƒëa th·ªã tr∆∞·ªùng
- C√≥ categorical features
- C·∫ßn t·ªëi ∆∞u hi·ªáu su·∫•t

#### **S·ª≠ d·ª•ng MultiRegionQCAAPatchTF khi:**
- Y√™u c·∫ßu m√¥ h√¨nh h√≥a theo v√πng c·ª• th·ªÉ
- Nghi√™n c·ª©u th·ª≠ nghi·ªám Phase 2
- C·∫ßn tham s·ªë v√πng t√πy ch·ªânh

### ‚ö†Ô∏è **L·ªói Th∆∞·ªùng g·∫∑p v√† Kh·∫Øc ph·ª•c:**

#### **V·∫•n ƒë·ªÅ Training:**
- **Overfitting**: S·ª≠ d·ª•ng dropout v√† early stopping
- **Gradient Explosion**: Clip gradients v·ªõi max_norm=1.0
- **Tr√†n B·ªô nh·ªõ**: Gi·∫£m batch size ho·∫∑c sequence length

#### **V·∫•n ƒë·ªÅ Ki·∫øn tr√∫c:**
- **K√≠ch th∆∞·ªõc Patch Sai**: ƒê·∫£m b·∫£o seq_len % patch_len == 0
- **Sai k√≠ch th∆∞·ªõc**: X√°c minh embedding dimensions
- **C·∫•u h√¨nh Th·ªã tr∆∞·ªùng**: Ki·ªÉm tra c_out kh·ªõp v·ªõi s·ªë th·ªã tr∆∞·ªùng

## üìã **T·ªïng k·∫øt**

C√°c ki·∫øn tr√∫c m√¥ h√¨nh trong folder n√†y ƒë·∫°i di·ªán cho c√¥ng ngh·ªá ti√™n ti·∫øn trong d·ª± b√°o supply chain, v·ªõi s·ª± c√¢n b·∫±ng t·ªët gi·ªØa ƒë·ªô ch√≠nh x√°c v√† hi·ªáu qu·∫£ t√≠nh to√°n. QCAAPatchTF_Embedding ƒë√£ ch·ª©ng minh hi·ªáu qu·∫£ v∆∞·ª£t tr·ªôi trong Phase 1 v·ªõi c·∫£i ti·∫øn 98.71% MSE v√† ƒë·∫°t 91.78% ƒë·ªô ch√≠nh x√°c, s·∫µn s√†ng cho tri·ªÉn khai s·∫£n xu·∫•t ho·∫∑c ph√°t tri·ªÉn Phase 2.
