# Layers - CÃ¡c ThÃ nh pháº§n Kiáº¿n trÃºc Model

## Tá»•ng quan
Folder `layers` chá»©a cÃ¡c building blocks cÆ¡ báº£n Ä‘á»ƒ xÃ¢y dá»±ng kiáº¿n trÃºc QTransformer. ÄÃ¢y lÃ  nÆ¡i Ä‘á»‹nh nghÄ©a cÃ¡c layer components: embedding layers, attention mechanisms, vÃ  encoder-decoder structures. CÃ¡c thÃ nh pháº§n nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ modular Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng vÃ  káº¿t há»£p linh hoáº¡t.

## Cáº¥u trÃºc ThÃ nh pháº§n

### ğŸ“Š **Embed.py** - Há»‡ thá»‘ng Embedding

#### ğŸ¯ **Má»¥c Ä‘Ã­ch chÃ­nh:**
Chuyá»ƒn Ä‘á»•i raw input data thÃ nh representation vectors phÃ¹ há»£p cho Transformer architecture, bao gá»“m value embeddings, positional encodings, vÃ  temporal features.

#### ğŸ§© **CÃ¡c Class chÃ­nh:**

##### **1. TokenEmbedding**
```python
class TokenEmbedding(nn.Module):
    def __init__(self, c_in, d_model):
        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                   kernel_size=3, padding=padding, padding_mode='circular')
```
**Chá»©c nÄƒng:** Chuyá»ƒn Ä‘á»•i input features thÃ nh token embeddings sá»­ dá»¥ng 1D convolution
**á»¨ng dá»¥ng:** Embedding cho numerical features trong time series data

##### **2. PositionalEmbedding**
```python
class PositionalEmbedding(nn.Module):
    def forward(self, x):
        return self.pe[:, :x.size(1)]  # Positional encoding sinusoid
```
**Chá»©c nÄƒng:** ThÃªm thÃ´ng tin vá»‹ trÃ­ cho sequence tokens
**Ã nghÄ©a:** GiÃºp model hiá»ƒu thá»© tá»± temporal trong time series

##### **3. TemporalEmbedding**
```python
class TemporalEmbedding(nn.Module):
    def forward(self, x):
        hour_x = self.hour_embed(x[:, :, 3])      # Hour of day
        weekday_x = self.weekday_embed(x[:, :, 2])  # Day of week  
        day_x = self.day_embed(x[:, :, 1])        # Day of month
        month_x = self.month_embed(x[:, :, 0])    # Month
        return hour_x + weekday_x + day_x + month_x
```
**Chá»©c nÄƒng:** Embedding cho temporal features (giá», ngÃ y, tuáº§n, thÃ¡ng)
**Lá»£i Ã­ch:** Capture seasonality patterns trong supply chain data

##### **4. DataEmbedding (Composite Class)**
```python
class DataEmbedding(nn.Module):
    def forward(self, x, x_mark):
        x = self.value_embedding(x) + \
            self.temporal_embedding(x_mark) + \
            self.position_embedding(x)
        return self.dropout(x)
```
**Chá»©c nÄƒng:** Káº¿t há»£p táº¥t cáº£ embedding types thÃ nh final representation
**Input:** Raw features (x) + time features (x_mark)
**Output:** Rich embedding vectors cho Transformer layers

##### **5. PatchEmbedding**
```python
class PatchEmbedding(nn.Module):
    def forward(self, x):
        # Patching process
        x = self.padding_patch_layer(x)
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
        x = self.value_embedding(x) + self.position_embedding(x)
        return self.dropout(x), n_vars
```
**Chá»©c nÄƒng:** Chia time series thÃ nh patches nhá», tÆ°Æ¡ng tá»± Vision Transformer
**Æ¯u Ä‘iá»ƒm:** Giáº£m computational complexity, tÄƒng locality awareness

#### ğŸ¨ **Embedding Variants:**

- **DataEmbedding_inverted**: Cho inverted architecture (feature-wise attention)
- **DataEmbedding_wo_pos**: KhÃ´ng sá»­ dá»¥ng positional embedding
- **TimeFeatureEmbedding**: Linear embedding cho continuous time features

### âš¡ **SelfAttention_Family.py** - Há» Attention Mechanisms

#### ğŸ¯ **Má»¥c Ä‘Ã­ch chÃ­nh:**
Cung cáº¥p cÃ¡c loáº¡i attention mechanisms khÃ¡c nhau Ä‘á»ƒ capture dependencies trong time series data, tá»« standard attention Ä‘áº¿n quantum-enhanced attention.

#### ğŸ§  **CÃ¡c Attention Types:**

##### **1. FullAttention (Standard)**
```python
class FullAttention(nn.Module):
    def forward(self, queries, keys, values, attn_mask):
        scores = torch.einsum("blhe,bshe->bhls", queries, keys)
        A = self.dropout(torch.softmax(scale * scores, dim=-1))
        V = torch.einsum("bhls,bshd->blhd", A, values)
```
**Äáº·c Ä‘iá»ƒm:** Classic scaled dot-product attention
**Sá»­ dá»¥ng:** Baseline attention cho standard Transformer

##### **2. DSAttention (De-stationary)**
```python
class DSAttention(nn.Module):
    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
        scores = torch.einsum("blhe,bshe->bhls", queries, keys) * tau + delta
```
**Äáº·c Ä‘iá»ƒm:** Attention vá»›i learnable de-stationary factors (tau, delta)
**á»¨ng dá»¥ng:** Xá»­ lÃ½ non-stationary patterns trong supply chain data
**Lá»£i Ã­ch:** Adaptive cho seasonal vÃ  trend changes

##### **3. ProbAttention (Sparse)**
```python
class ProbAttention(nn.Module):
    def _prob_QK(self, Q, K, sample_k, n_top):
        # Probability-based key sampling
        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)
```
**Äáº·c Ä‘iá»ƒm:** Chá»‰ compute attention cho top-k most relevant pairs
**Æ¯u Ä‘iá»ƒm:** Giáº£m complexity tá»« O(LÂ²) xuá»‘ng O(L log L)
**Sá»­ dá»¥ng:** Long sequence forecasting

##### **4. QuantumAttention (Experimental)** ğŸš€
```python
class QuantumAttention(nn.Module):
    def variational_circuit(self, params):
        # Quantum circuit vá»›i VQE
        for i in range(self.num_qubits):
            qml.RY(params[i], wires=i)
        for i in range(self.num_qubits - 1):
            qml.CNOT(wires=[i, i + 1])
        return qml.expval(qml.PauliZ(0))
```
**Äáº·c Ä‘iá»ƒm:** Sá»­ dá»¥ng Variational Quantum Eigensolver cho attention computation
**Innovation:** Quantum entanglement Ä‘á»ƒ capture complex dependencies
**Status:** Research experiment cho Phase 2

##### **5. AttentionLayer (Wrapper)**
```python
class AttentionLayer(nn.Module):
    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
        # Multi-head projections
        queries = self.query_projection(queries).view(B, L, H, -1)
        keys = self.key_projection(keys).view(B, S, H, -1)
        values = self.value_projection(values).view(B, S, H, -1)
        
        # Apply attention mechanism
        out, attn = self.inner_attention(queries, keys, values, attn_mask, tau, delta)
        return self.out_projection(out.view(B, L, -1)), attn
```
**Chá»©c nÄƒng:** Wrapper Ä‘á»ƒ implement multi-head attention vá»›i báº¥t ká»³ attention type nÃ o

#### ğŸ”„ **Specialized Layers:**

- **TwoStageAttentionLayer**: Two-stage attention cho hierarchical patterns
- **TimeAttention**: Time-aware attention vá»›i temporal weighting

### ğŸ—ï¸ **Transformer_EncDec.py** - Encoder-Decoder Architecture

#### ğŸ¯ **Má»¥c Ä‘Ã­ch chÃ­nh:**
Äá»‹nh nghÄ©a cÃ¡c building blocks cho Transformer encoder-decoder architecture, bao gá»“m cÃ¡c layer components vÃ  stacking logic.

#### ğŸ§© **Core Components:**

##### **1. EncoderLayer**
```python
class EncoderLayer(nn.Module):
    def forward(self, x, attn_mask=None, tau=None, delta=None):
        # Self-attention
        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask, tau=tau, delta=delta)
        x = x + self.dropout(new_x)  # Residual connection
        
        # Feed-forward network
        y = self.norm1(x)
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))
        
        return self.norm2(x + y), attn  # Layer normalization
```
**Cáº¥u trÃºc:** Self-attention + Feed-forward + Residual connections + Layer normalization
**Chá»©c nÄƒng:** Encode input sequence thÃ nh rich representations

##### **2. Encoder (Stack)**
```python
class Encoder(nn.Module):
    def forward(self, x, attn_mask=None, tau=None, delta=None):
        attns = []
        for attn_layer in self.attn_layers:
            x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
            attns.append(attn)
        return x, attns
```
**Chá»©c nÄƒng:** Stack nhiá»u EncoderLayers Ä‘á»ƒ táº¡o deep representations
**Flexibility:** Support cáº£ convolutional layers cho downsampling

##### **3. DecoderLayer**
```python
class DecoderLayer(nn.Module):
    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):
        # Self-attention trÃªn decoder sequence
        x = x + self.dropout(self.self_attention(x, x, x, attn_mask=x_mask)[0])
        x = self.norm1(x)
        
        # Cross-attention vá»›i encoder output
        x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])
        
        # Feed-forward network
        y = x = self.norm2(x)
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))
        
        return self.norm3(x + y)
```
**Cáº¥u trÃºc:** Self-attention + Cross-attention + Feed-forward
**Chá»©c nÄƒng:** Generate output sequence dá»±a trÃªn encoder representations

##### **4. Decoder (Stack)**
```python
class Decoder(nn.Module):
    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):
        for layer in self.layers:
            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, tau=tau, delta=delta)
        
        if self.projection is not None:
            x = self.projection(x)  # Final output projection
        return x
```
**Chá»©c nÄƒng:** Stack nhiá»u DecoderLayers + final projection
**Output:** Final forecasting results

##### **5. ConvLayer (Auxiliary)**
```python
class ConvLayer(nn.Module):
    def forward(self, x):
        x = self.downConv(x.permute(0, 2, 1))    # 1D convolution
        x = self.norm(x)                         # Batch normalization
        x = self.activation(x)                   # ELU activation
        x = self.maxPool(x)                      # Max pooling
        return x.transpose(1, 2)
```
**Chá»©c nÄƒng:** Downsampling layer cho hierarchical attention
**á»¨ng dá»¥ng:** Giáº£m sequence length trong deep encoders

## TÃ­ch há»£p vá»›i QTransformer

### ğŸ”— **Component Integration:**

#### **Phase 1 Architecture Stack:**
```python
# 1. Data Embedding
embedding = DataEmbedding(c_in=features, d_model=512)

# 2. Encoder Stack
encoder_layers = [
    EncoderLayer(
        attention=AttentionLayer(DSAttention(), d_model=512, n_heads=8),
        d_model=512, d_ff=2048
    ) for _ in range(6)
]
encoder = Encoder(encoder_layers)

# 3. Decoder Stack  
decoder_layers = [
    DecoderLayer(
        self_attention=AttentionLayer(FullAttention(), d_model=512, n_heads=8),
        cross_attention=AttentionLayer(FullAttention(), d_model=512, n_heads=8),
        d_model=512, d_ff=2048
    ) for _ in range(3)
]
decoder = Decoder(decoder_layers, projection=nn.Linear(512, c_out))
```

#### **Embedding cho Supply Chain:**
```python
# Supply chain specific embedding
data_embedding = DataEmbedding(
    c_in=21,           # 21 features tá»« processed data
    d_model=512,       # Hidden dimension
    embed_type='fixed', # Fixed temporal embedding
    freq='d',          # Daily frequency
    dropout=0.1
)

# Input: [batch_size, seq_len, 21], [batch_size, seq_len, 4]
# Output: [batch_size, seq_len, 512]
```

### ğŸ¯ **Attention Strategy cho Multi-Market:**

#### **DSAttention cho Non-stationary Supply Chain:**
- **tau parameter**: Há»c seasonal scaling factors
- **delta parameter**: Há»c trend adjustment factors  
- **Multi-head**: 8 heads Ä‘á»ƒ capture different temporal patterns

#### **QuantumAttention Research Direction:**
- **Entanglement**: Model cross-market dependencies
- **VQE optimization**: Learnable quantum parameters
- **Hybrid approach**: Classical + Quantum attention combination

## Performance Characteristics

### âš¡ **Computational Complexity:**

| Attention Type | Time Complexity | Space Complexity | Use Case |
|---------------|----------------|------------------|----------|
| FullAttention | O(LÂ²) | O(LÂ²) | Standard sequences |
| ProbAttention | O(L log L) | O(L log L) | Long sequences |
| DSAttention | O(LÂ²) | O(LÂ²) | Non-stationary data |
| QuantumAttention | O(LÂ²) | O(LÂ²) | Research experiments |

### ğŸ“Š **Phase 1 Usage:**
- **Primary**: DSAttention vá»›i TemporalEmbedding
- **Embedding**: DataEmbedding vá»›i 21 features + 4 time features
- **Architecture**: 6 encoder layers + 3 decoder layers
- **Results**: 98.71% MSE cáº£i thiá»‡n vá»›i attention mechanisms nÃ y

## HÆ°á»›ng phÃ¡t triá»ƒn Phase 2

### ğŸš€ **Planned Enhancements:**

#### 1. **Advanced Embedding:**
- **Categorical Embeddings**: TÃ­ch há»£p Market, Category embeddings  
- **External Features**: Weather, holiday, economic indicators
- **Dynamic Embedding**: Adaptive embedding dimensions

#### 2. **Attention Innovations:**
- **Multi-scale Attention**: Capture patterns á»Ÿ multiple time scales
- **Cross-market Attention**: Explicit cross-region dependencies
- **Adaptive Sparsity**: Dynamic attention sparsity patterns

#### 3. **Architecture Optimization:**
- **Efficient Transformers**: Linformer, Performer variants
- **Memory Efficient**: Gradient checkpointing, mixed precision
- **Model Parallelism**: Distributed training support

### ğŸ”¬ **Research Directions:**
- **Quantum Integration**: Expand QuantumAttention vá»›i real quantum hardware
- **Neuromorphic Attention**: Brain-inspired attention mechanisms
- **Causal Discovery**: Attention-based causal inference

## HÆ°á»›ng dáº«n Sá»­ dá»¥ng

### ğŸ› ï¸ **Custom Attention Implementation:**
```python
# Táº¡o custom attention type
from layers.SelfAttention_Family import AttentionLayer

# Initialize attention
custom_attention = AttentionLayer(
    attention=DSAttention(mask_flag=True, scale=None, attention_dropout=0.1),
    d_model=512,
    n_heads=8
)

# Sá»­ dá»¥ng trong EncoderLayer
encoder_layer = EncoderLayer(
    attention=custom_attention,
    d_model=512,
    d_ff=2048,
    dropout=0.1
)
```

### ğŸ”§ **Debugging Attention:**
```python
# Kiá»ƒm tra attention weights
_, attention_weights = attention_layer(x, x, x, output_attention=True)
print(f"Attention shape: {attention_weights.shape}")  # [B, H, L, S]

# Visualize attention patterns
import matplotlib.pyplot as plt
plt.imshow(attention_weights[0, 0].detach().cpu(), cmap='Blues')
plt.title('Attention Pattern Head 0')
```

### âš ï¸ **Common Issues:**
- **Memory errors**: Giáº£m sequence length hoáº·c sá»­ dá»¥ng ProbAttention
- **NaN values**: Kiá»ƒm tra attention mask vÃ  scaling factor
- **Slow training**: Sá»­ dá»¥ng mixed precision vÃ  efficient attention variants

Layers folder nÃ y cung cáº¥p foundation building blocks Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u cho supply chain forecasting, vá»›i flexibility cao Ä‘á»ƒ má»Ÿ rá»™ng cho Phase 2. Architecture modular giÃºp dá»… dÃ ng experiment vá»›i different attention mechanisms vÃ  embedding strategies.
