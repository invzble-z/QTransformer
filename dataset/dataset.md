# Dataset - Kho Dá»¯ liá»‡u Dá»± Ã¡n QTransformer

## THIáº¾U FILE DataCoSupplyChain_Synchronized.csv 82MB khÃ´ng push Ä‘Æ°á»£c
## Tá»•ng quan
Folder `dataset` chá»©a toÃ n bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng trong dá»± Ã¡n QTransformer, tá»« raw data ban Ä‘áº§u Ä‘áº¿n cÃ¡c processed datasets Ä‘Ã£ Ä‘Æ°á»£c optimize cho training model. ÄÃ¢y lÃ  káº¿t quáº£ cá»§a quÃ¡ trÃ¬nh data preprocessing phá»©c táº¡p biáº¿n Ä‘á»•i **147,041 transactions** thÃ nh **765 time series records**.

## Cáº¥u trÃºc Dá»¯ liá»‡u

### ğŸ“Š **File Data chÃ­nh**

#### 1. **DataCoSupplyChain_Synchronized.csv** (82.2MB - Raw Data)
- **Nguá»“n gá»‘c**: Raw dataset tá»« DataCo Supply Chain cho Analysis
- **KÃ­ch thÆ°á»›c**: 147,042 records (147,041 transactions + header)
- **Timeline**: 2017-01-18 Ä‘áº¿n 2018-01-31 (379 ngÃ y)
- **Pháº¡m vi**: Global supply chain data vá»›i multiple markets
- **Má»¥c Ä‘Ã­ch**: Source data cho toÃ n bá»™ pipeline preprocessing

**Äáº·c Ä‘iá»ƒm:**
- Multi-market data (Europe, LATAM, USCA)
- Transaction-level granularity
- Raw categorical vÃ  numerical features
- Unprocessed timestamps vÃ  business metrics

#### 2. **supply_chain_processed.csv** (203KB - Processed Data)
- **Nguá»“n gá»‘c**: Output chÃ­nh tá»« data preprocessing pipeline  
- **KÃ­ch thÆ°á»›c**: 766 records (765 + header)
- **Timeline**: 2017-05-22 Ä‘áº¿n 2018-01-31 (255 ngÃ y)
- **Transformation**: Transaction â†’ Daily aggregated time series
- **Má»¥c Ä‘Ã­ch**: Primary dataset cho model training

**Key Features (22 columns):**
```
- order_date_only: Date index
- Market: Categorical (Europe/LATAM/USCA) 
- Market_encoded: Encoded version (0/1/2)
- Target: order_count (daily aggregated orders)
- Business metrics: shipping days, delivery risk, prices, profits
- Customer metrics: segment distributions, category diversity
- Time features: day_of_week, month, is_weekend, days_since_start
```

#### 3. **supply_chain_optimized.csv** (552KB - Enhanced Data)
- **Nguá»“n gá»‘c**: Advanced version cá»§a processed data vá»›i feature engineering
- **KÃ­ch thÆ°á»›c**: 766 records vá»›i 50 features (28 features bá»• sung)
- **Enhancement**: Cyclical encoding, market analytics, volatility metrics
- **Má»¥c Ä‘Ã­ch**: Experimental dataset cho advanced model architectures

**Advanced Features bá»• sung:**
```
- Cyclical encoding: month_sin/cos, day_sin/cos, week_sin/cos
- Market analytics: market ratios, shares, volatilities  
- Trend analysis: 7-day trends, momentum indicators
- Temporal features: is_month_end, is_quarter_end, week_of_year
```

### ğŸ“‹ **Metadata Files**

#### 4. **feature_mapping.json** (Cáº¥u hÃ¬nh Features)
```json
{
  "total_features": 21,
  "numerical_features": 20,
  "categorical_features": 1,
  "target_column": "order_count",
  "market_encoding": {"Europe": "0", "LATAM": "1", "USCA": "2"}
}
```

**Má»¥c Ä‘Ã­ch:**
- Document feature structure cho model configuration
- Mapping categorical values Ä‘á»ƒ consistency
- Reference cho data validation vÃ  debugging

#### 5. **preprocessing_stats.json** (Thá»‘ng kÃª Processing)
```json
{
  "original_records": 147041,
  "processed_records": 765,
  "data_loss_percentage": 11.712379540400295,
  "synchronized_timeline": "2017-05-22 to 2018-01-31",
  "avg_orders_per_day_per_market": 170.92
}
```

**Má»¥c Ä‘Ã­ch:**
- Track data transformation metrics
- Quality assurance cho preprocessing pipeline  
- Performance benchmarks cho Phase 2 optimization

## Data Transformation Pipeline

### ğŸ”„ **Raw â†’ Processed Workflow**

#### Stage 1: Data Synchronization
```
Input: 147,041 transactions (2017-01-18 to 2018-01-31)
Process: Market alignment, outlier removal, date filtering
Output: Synchronized dataset (2017-05-22 to 2018-01-31)
Data Loss: ~11.7% (quality improvement trade-off)
```

#### Stage 2: Temporal Aggregation  
```
Grouping: Daily aggregation per market
Transformation: Transaction-level â†’ Time series
Features: Business metrics averaging, counting, statistical analysis
Result: 765 time series points (255 days Ã— 3 markets)
```

#### Stage 3: Feature Engineering
```
Categorical Encoding: Market â†’ numeric codes
Time Features: Extract day_of_week, month, weekend flags
Business Analytics: Customer segments, category diversity, volatility
Normalization: Price metrics, profit ratios
```

### ğŸ“ˆ **Processed â†’ Optimized Enhancement**

#### Advanced Feature Engineering:
- **Cyclical Encoding**: Sin/cos transformation cho temporal features
- **Market Analytics**: Cross-market ratios, market shares, volatilities
- **Trend Analysis**: Moving averages, momentum indicators  
- **Temporal Enrichment**: Quarter/month end flags, week numbers

## Data Quality & Validation

### âœ… **Quality Metrics**
- **Completeness**: 100% data availability sau synchronization
- **Consistency**: Uniform date ranges across markets
- **Accuracy**: Business logic validation (profits, quantities)
- **Temporal Integrity**: Sequential dates without gaps

### ğŸ” **Validation Checks**
```python
# Data shape validation
assert len(supply_chain_processed) == 765
assert len(supply_chain_optimized) == 765

# Feature count validation  
assert supply_chain_processed.shape[1] == 22  # 21 features + date
assert supply_chain_optimized.shape[1] == 50  # Enhanced features

# Market encoding validation
assert set(data['Market_encoded']) == {0, 1, 2}
assert data['Market_encoded'].nunique() == 3
```

## Usage Guidelines

### ğŸ¯ **Dataset Selection Strategy**

#### **supply_chain_processed.csv** - Sá»­ dá»¥ng khi:
- Standard training cho baseline models
- Quick experiments vÃ  prototyping  
- Memory-limited environments
- Production deployment vá»›i stable performance

#### **supply_chain_optimized.csv** - Sá»­ dá»¥ng khi:
- Advanced model architectures (QCAAPatchTF_Embedding)
- Feature engineering experiments
- Performance optimization projects
- Research phase vá»›i sophisticated features

### ğŸš€ **Loading Examples**

#### Basic Loading:
```python
import pandas as pd

# Load processed data
df_processed = pd.read_csv('dataset/supply_chain_processed.csv')
df_processed['order_date_only'] = pd.to_datetime(df_processed['order_date_only'])

# Load optimized data  
df_optimized = pd.read_csv('dataset/supply_chain_optimized.csv')
df_optimized['order_date_only'] = pd.to_datetime(df_optimized['order_date_only'])
```

#### Integration vá»›i Data Provider:
```python
# Trong run.py
args.data_path = 'supply_chain_processed.csv'  # Standard approach
# args.data_path = 'supply_chain_optimized.csv'  # Advanced approach

# Data provider tá»± Ä‘á»™ng detect vÃ  load
train_data, train_loader = data_provider(args, flag='train')
```

## Performance Benchmarks

### ğŸ“Š **Phase 1 Results**

#### **supply_chain_processed.csv**:
- **MSE Improvement**: 98.71% vs baseline
- **Accuracy**: 91.78% 
- **Training Time**: ~15 minutes/epoch
- **Memory Usage**: ~2GB peak

#### **supply_chain_optimized.csv** (experimental):
- **Feature richness**: 2.27x more features
- **Model complexity**: Higher embedding dimensions
- **Potential**: Enhanced pattern recognition capability

### âš¡ **Performance Characteristics**
```
File I/O Speed:
- processed.csv: ~50ms load time
- optimized.csv: ~120ms load time (more features)

Memory Footprint:
- processed: ~15MB in memory
- optimized: ~35MB in memory

Training Compatibility:
- processed: All model architectures  
- optimized: Embedding-based models preferred
```

## Data Lineage & Versioning

### ğŸ”„ **Version History** (Ä‘Ã£ cleanup)
- ~~supply_chain_processed_v1.csv~~ (deleted - outdated)
- ~~supply_chain_processed_v2.csv~~ (deleted - outdated) 
- **supply_chain_processed.csv** (current stable version)
- **supply_chain_optimized.csv** (current enhanced version)

### ğŸ“ **Change Tracking**
- **v1 â†’ v2**: Minor order_count adjustments
- **v2 â†’ current**: Final validation vÃ  business logic fixes
- **processed â†’ optimized**: Major feature engineering enhancement

## Phase 2 Data Roadmap

### ğŸ¯ **Planned Enhancements**

#### 1. **External Data Integration**
- Weather data correlation  
- Economic indicators (GDP, inflation)
- Holiday calendars per market
- Competitor pricing data

#### 2. **Advanced Feature Engineering**
- Fourier transform features cho seasonality
- Lag features vá»›i automated selection
- Market cross-correlation features
- Supply chain disruption indicators

#### 3. **Data Pipeline Optimization**
- Real-time data ingestion capability
- Automated data quality monitoring
- Feature store implementation
- Version control cho datasets

#### 4. **Scalability Improvements**
- Partitioned datasets cho large-scale training
- Streaming data processing
- Cloud storage integration
- Distributed training support

## Troubleshooting

### â— **Common Issues**

#### Data Loading Problems:
```python
# Fix date parsing issues
df['order_date_only'] = pd.to_datetime(df['order_date_only'], format='%Y-%m-%d')

# Handle missing values
assert df.isnull().sum().sum() == 0  # Should be 0

# Validate market encoding
assert df['Market_encoded'].isin([0, 1, 2]).all()
```

#### Memory Issues:
```python
# Efficient loading cho large datasets
df = pd.read_csv('dataset/supply_chain_optimized.csv', 
                 dtype={'Market_encoded': 'int8',
                        'is_weekend': 'int8',
                        'day_of_week': 'int8'})
```

#### Feature Mismatch:
```python
# Verify feature compatibility
expected_features = json.load(open('dataset/feature_mapping.json'))
assert len(df.columns) == expected_features['total_features'] + 1  # +1 for date
```

Dataset folder nÃ y chá»©a foundation data Ä‘Ã£ Ä‘Æ°á»£c validate ká»¹ lÆ°á»¡ng vÃ  optimize cho **98.71% MSE improvement** trong Phase 1. Cáº¥u trÃºc data clean, documentation Ä‘áº§y Ä‘á»§, sáºµn sÃ ng cho Phase 2 expansion.
