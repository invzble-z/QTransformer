# Experiment Management - Há»‡ thá»‘ng Quáº£n lÃ½ ThÃ­ nghiá»‡m

## Tá»•ng quan
Folder `exp` chá»©a há»‡ thá»‘ng experiment management theo **Factory Pattern**, chá»‹u trÃ¡ch nhiá»‡m orchestrate toÃ n bá»™ quÃ¡ trÃ¬nh training, validation vÃ  testing cá»§a models. ÄÃ¢y lÃ  layer controller chÃ­nh káº¿t ná»‘i args configuration, data providers, models vÃ  evaluation metrics.

## Kiáº¿n trÃºc Há»‡ thá»‘ng

### ğŸ—ï¸ **Hierarchy Design Pattern**

```
Exp_Basic (Abstract Base Class)
    â”œâ”€â”€ Exp_Long_Term_Forecast (Standard Approach)
    â””â”€â”€ Exp_Long_Term_Forecast_Embedding (Embedding Approach - Phase 1)
```

**Philosophy:** Separation of concerns vá»›i shared infrastructure vÃ  specialized implementations

## Chi tiáº¿t Implementations

### 1. **exp_basic.py** - Foundation Layer

#### ğŸ§© **Core Responsibilities:**
- **Model Registry**: Central repository cho táº¥t cáº£ available models
- **Device Management**: GPU/CPU allocation vÃ  multi-GPU support  
- **Abstract Interface**: Template cho experiment workflows

#### ğŸ“‹ **Model Dictionary:**
```python
self.model_dict = {
    # Legacy models (commented out - chá»‰ reference)
    'TimesNet': None, 'Autoformer': None, 'Transformer': None,
    
    # Active models - Phase 1
    'QCAAPatchTF': QCAAPatchTF,                    # Standard approach
    'QCAAPatchTF_Embedding': QCAAPatchTF_Embedding # Embedding approach â­
}
```

**Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t:**
- **Dynamic Model Loading**: Import models on-demand Ä‘á»ƒ tiáº¿t kiá»‡m memory
- **GPU Auto-detection**: Tá»± Ä‘á»™ng configure CUDA devices
- **Multi-GPU Support**: DataParallel cho large models
- **Abstract Methods**: `_build_model()`, `train()`, `test()`, `vali()` must be implemented

#### ğŸ”§ **Device Management Logic:**
```python
def _acquire_device(self):
    if self.args.use_gpu:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(self.args.gpu)
        device = torch.device('cuda:{}'.format(self.args.gpu))
    else:
        device = torch.device('cpu')
```

### 2. **exp_long_term_forecasting.py** - Standard Experiment Class

#### ğŸ¯ **Purpose:** Baseline experiment class cho standard models (non-embedding)

#### ğŸš€ **Key Features:**
- **Standard PyTorch Pipeline**: Train/Val/Test loop vá»›i best practices
- **Early Stopping**: Patience-based stopping Ä‘á»ƒ trÃ¡nh overfitting
- **Learning Rate Scheduling**: Dynamic LR adjustment
- **Mixed Precision**: AMP support cho faster training
- **Result Persistence**: Save predictions, metrics, visualizations

#### ğŸ“Š **Training Loop Architecture:**
```python
# Training workflow
for epoch in range(train_epochs):
    # Training phase
    for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:
        # 1. Prepare decoder input
        dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :])
        dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1)
        
        # 2. Model forward pass
        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
        
        # 3. Loss calculation
        loss = criterion(outputs[:, -pred_len:, :], batch_y[:, -pred_len:, :])
        
        # 4. Backpropagation
        loss.backward()
        optimizer.step()
    
    # Validation phase
    vali_loss = self.vali(vali_data, vali_loader, criterion)
    
    # Early stopping check
    early_stopping(vali_loss, model, path)
```

#### ğŸ“ˆ **Testing vÃ  Results:**
- **Prediction Generation**: Generate forecasts cho test period
- **Metrics Calculation**: MSE, MAE, RMSE, MAPE, MSPE
- **Result Visualization**: PDF plots cho prediction vs ground truth
- **File Persistence**: Save predictions as CSV vá»›i timestamps

### 3. **exp_long_term_forecasting_embedding.py** - Advanced Experiment Class â­

#### ğŸ¯ **Purpose:** Specialized experiment class cho embedding-based models (Phase 1 chÃ­nh)

#### ğŸŒŸ **Advanced Features:**

##### **WeightedMSELoss Implementation:**
```python
class WeightedMSELoss(nn.Module):
    def __init__(self, market_weights=None):
        # Phase 1 roadmap weights: [Europe=0.35, LATAM=0.30, USCA=0.35]
        self.market_weights = torch.tensor([0.35, 0.30, 0.35])
    
    def forward(self, predictions, targets):
        # predictions/targets: [batch_size, pred_len, 3_markets]
        mse_per_market = torch.mean((predictions - targets) ** 2, dim=(0, 1))
        weighted_loss = torch.sum(self.market_weights * mse_per_market)
```

**LÃ½ do sá»­ dá»¥ng:** Dá»± bÃ¡o multi-market vá»›i trá»ng sá»‘ Æ°u tiÃªn kinh doanh

##### **Xá»­ lÃ½ Dá»¯ liá»‡u Embedding:**
```python
# Xá»­ lÃ½ batch linh hoáº¡t
if len(batch_data) == 5:  # Dá»¯ liá»‡u embedding
    batch_x, batch_y, batch_x_mark, batch_y_mark, categorical_features = batch_data
    categorical_features = {k: v.to(device) for k, v in categorical_features.items()}
    
    # Model forward vá»›i categorical features
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, categorical_features)
else:  # Dá»¯ liá»‡u standard fallback
    batch_x, batch_y, batch_x_mark, batch_y_mark = batch_data
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
```

##### **TÃ­ch há»£p Target Scaler:**
```python
# Load target scaler Ä‘á»ƒ denormalization
self.target_scaler = joblib.load('./scalers/target_scaler.pkl')

def _calculate_corrected_metrics(self, preds, trues):
    # Denormalize predictions vÃ  ground truth
    preds_denorm = self.target_scaler.inverse_transform(preds.reshape(-1, 1))
    trues_denorm = self.target_scaler.inverse_transform(trues.reshape(-1, 1))
    
    # TÃ­nh toÃ¡n business metrics trÃªn actual scale
    mae = np.mean(np.abs(preds_denorm - trues_denorm))
    mse = np.mean((preds_denorm - trues_denorm) ** 2)
```

**Ã nghÄ©a:** TÃ­nh toÃ¡n metrics trÃªn scale kinh doanh thay vÃ¬ normalized scale

#### ğŸ¯ **Tá»‘i Æ°u hÃ³a Phase 1:**

##### **Há»— trá»£ Multi-Market:**
- **Market-aware Loss**: Weighted loss theo táº§m quan trá»ng kinh doanh
- **Metrics theo VÃ¹ng**: Theo dÃµi hiá»‡u suáº¥t tá»«ng thá»‹ trÆ°á»ng
- **Categorical Embeddings**: Embeddings Region, Category cho nháº­n diá»‡n pattern

##### **TÃ­nh nÄƒng Production-Ready:**
- **Xá»­ lÃ½ Lá»—i Robust**: CÆ¡ cháº¿ fallback cho scalers thiáº¿u
- **Loading Dá»¯ liá»‡u Linh hoáº¡t**: Há»— trá»£ cáº£ embedding vÃ  non-embedding datasets
- **Tá»‘i Æ°u Memory**: CÃ¡c phÃ©p toÃ¡n tensor hiá»‡u quáº£ cho batch lá»›n
- **Logging Chi tiáº¿t**: Theo dÃµi tiáº¿n trÃ¬nh chi tiáº¿t vÃ  debug info

## TÃ­ch há»£p Workflow

### ğŸ”„ **Pipeline End-to-End:**

#### 1. **Giai Ä‘oáº¡n Khá»Ÿi táº¡o:**
```python
# Trong run.py
exp = Exp_Long_Term_Forecast_Embedding(args)  # Factory táº¡o experiment
```

#### 2. **Giai Ä‘oáº¡n Training:**
```python
# Thá»±c thi training
model = exp.train(setting)  # Tráº£ vá» model Ä‘Ã£ train
```

#### 3. **Giai Ä‘oáº¡n Testing:**
```python
# Testing vÃ  Ä‘Ã¡nh giÃ¡
exp.test(setting, test=1)  # Táº¡o predictions vÃ  metrics
```

#### 4. **Giai Ä‘oáº¡n Káº¿t quáº£:**
```
Cáº¥u trÃºc Output:
â”œâ”€â”€ checkpoints/{setting}/checkpoint.pth     # Trá»ng sá»‘ model tá»‘t nháº¥t
â”œâ”€â”€ test_results/{setting}/                  # Káº¿t quáº£ visual
â”‚   â”œâ”€â”€ predictions.csv                      # Káº¿t quáº£ dá»± bÃ¡o
â”‚   â”œâ”€â”€ metrics.npy                          # Metrics hiá»‡u suáº¥t
â”‚   â””â”€â”€ *.pdf                                # Biá»ƒu Ä‘á»“ visualization
â””â”€â”€ results/{setting}/                       # Káº¿t quáº£ Ä‘Ã£ xá»­ lÃ½
    â”œâ”€â”€ pred.npy, true.npy                   # Predictions thÃ´
    â””â”€â”€ metrics_corrected.npy                # Metrics Ä‘Ã£ denormalized
```

## Quáº£n lÃ½ Cáº¥u hÃ¬nh

### âš™ï¸ **TÃ­ch há»£p Arguments chÃ­nh:**

#### **Lá»±a chá»n Model:**
```python
args.model = 'QCAAPatchTF_Embedding'  # Model chÃ­nh Phase 1
args.data = 'SupplyChainEmbedding'     # Dataset embedding
```

#### **Cáº¥u hÃ¬nh Training:**
```python
args.train_epochs = 10        # Sá»‘ epoch training
args.patience = 3             # Patience cho early stopping
args.learning_rate = 0.0001   # Learning rate ban Ä‘áº§u
args.batch_size = 32          # KÃ­ch thÆ°á»›c batch cho training
```

#### **Cáº¥u hÃ¬nh Kiáº¿n trÃºc:**
```python
args.seq_len = 7      # Äá»™ dÃ i input sequence (7 ngÃ y)
args.label_len = 1    # Äá»™ dÃ i decoder input
args.pred_len = 1     # Horizon dá»± bÃ¡o (1 ngÃ y)
args.c_out = 3        # Sá»‘ output channels (3 thá»‹ trÆ°á»ng)
```

## Theo dÃµi Hiá»‡u suáº¥t

### ğŸ“Š **ThÃ nh tá»±u Phase 1:**

#### **So sÃ¡nh Metrics:**
```
Baseline vs QCAAPatchTF_Embedding:
- Cáº£i thiá»‡n MSE: 98.71%
- Äá»™ chÃ­nh xÃ¡c: 91.78%
- Thá»i gian Training: ~15 phÃºt/epoch
- Sá»­ dá»¥ng Memory: ~3GB peak
```

#### **TÃ¡c Ä‘á»™ng Kinh doanh:**
- **Dá»± bÃ¡o Äa thá»‹ trÆ°á»ng**: Dá»± bÃ¡o Ä‘á»“ng thá»i cho Europe/LATAM/USCA
- **Categorical Intelligence**: Há»c pattern tá»« vÃ¹ng vÃ  category
- **Metrics Thá»±c táº¿**: ÄÃ¡nh giÃ¡ trÃªn scale kinh doanh thá»±c
- **Triá»ƒn khai Production**: Xá»­ lÃ½ lá»—i robust vÃ  fallback mechanisms

### ğŸ” **TÃ­nh nÄƒng GiÃ¡m sÃ¡t:**

#### **GiÃ¡m sÃ¡t Training:**
```python
# Theo dÃµi tiáº¿n trÃ¬nh real-time
print(f"Epoch: {epoch}, Train Loss: {train_loss:.7f}, Vali Loss: {vali_loss:.7f}")
print(f"Tá»‘c Ä‘á»™: {speed:.4f}s/iter, Thá»i gian cÃ²n láº¡i: {left_time:.4f}s")
```

#### **LÆ°u trá»¯ Káº¿t quáº£:**
```python
# Ghi log káº¿t quáº£ toÃ n diá»‡n
f.write(f'{setting}\n')
f.write(f'mse:{mse}, mae:{mae}, rmse:{rmse}, mape:{mape}, mspe:{mspe}\n')
```

## Káº¿ hoáº¡ch PhÃ¡t triá»ƒn Phase 2

### ğŸš€ **CÃ¡c Cáº£i tiáº¿n Dá»± kiáº¿n:**

#### 1. **Loáº¡i Experiment NÃ¢ng cao:**
- **Exp_Multi_Task_Forecast**: Dá»± bÃ¡o Ä‘a nhiá»‡m vá»¥ Ä‘á»“ng thá»i
- **Exp_Federated_Learning**: Training phÃ¢n tÃ¡n theo vÃ¹ng
- **Exp_Real_Time_Inference**: Há»c online vÃ  thÃ­ch á»©ng real-time

#### 2. **GiÃ¡m sÃ¡t NÃ¢ng cao:**
- **TensorBoard Integration**: Trá»±c quan hÃ³a thá»i gian thá»±c
- **MLflow Tracking**: Quáº£n lÃ½ version vÃ  so sÃ¡nh experiment
- **Distributed Logging**: GiÃ¡m sÃ¡t táº­p trung cho production

#### 3. **TÃ­ch há»£p AutoML:**
- **Tá»‘i Æ°u Hyperparameter**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh tham sá»‘
- **TÃ¬m kiáº¿m Kiáº¿n trÃºc**: Neural architecture search tá»± Ä‘á»™ng
- **Chá»n lá»c Feature**: Feature engineering tá»± Ä‘á»™ng

#### 4. **TÃ­nh nÄƒng Production:**
- **Model Serving**: TÃ­ch há»£p FastAPI
- **A/B Testing**: Framework so sÃ¡nh experiment
- **Dashboard GiÃ¡m sÃ¡t**: Theo dÃµi hiá»‡u suáº¥t real-time

## HÆ°á»›ng dáº«n Kháº¯c phá»¥c Sá»± cá»‘

### â— **CÃ¡c Váº¥n Ä‘á» ThÆ°á»ng gáº·p:**

#### **Lá»—i Memory:**
```python
# Giáº£i phÃ¡p: Giáº£m batch size hoáº·c báº­t gradient checkpointing
args.batch_size = 16  # Giáº£m tá»« 32
args.use_amp = True   # Báº­t mixed precision
```

#### **Váº¥n Ä‘á» Há»™i tá»¥:**
```python
# Giáº£i phÃ¡p: Äiá»u chá»‰nh learning rate vÃ  patience
args.learning_rate = 0.0001  # LR tháº¥p hÆ¡n Ä‘á»ƒ á»•n Ä‘á»‹nh
args.patience = 5            # TÄƒng patience
```

#### **Lá»—i Categorical Feature:**
```python
# Kiá»ƒm tra cáº¥u trÃºc embedding data
assert len(batch_data) == 5, "Thiáº¿u categorical features"
assert 'Market' in categorical_features, "Thiáº¿u Market embeddings"
```

### ğŸ”§ **Lá»‡nh Debug:**
```python
# Kiá»ƒm tra model
print(f"Sá»‘ tham sá»‘ model: {sum(p.numel() for p in model.parameters())}")

# Kiá»ƒm tra data  
print(f"KÃ­ch thÆ°á»›c batch: {batch_x.shape}, Categorical keys: {categorical_features.keys()}")

# Kiá»ƒm tra training
print(f"Xu hÆ°á»›ng loss: {train_loss[-10:]}")  # 10 loss cuá»‘i
```

Há»‡ thá»‘ng experiment management nÃ y Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u qua Phase 1 vá»›i **cáº£i thiá»‡n MSE 98.71%**. Kiáº¿n trÃºc sáº¡ch, dá»… má»Ÿ rá»™ng vÃ  sáºµn sÃ ng cho phÃ¡t triá»ƒn Phase 2.
