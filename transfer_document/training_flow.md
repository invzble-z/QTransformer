# üîß TRAINING FLOW - H∆Ø·ªöNG D·∫™N CHI TI·∫æT QU√Å TR√åNH HU·∫§N LUY·ªÜN

## üéØ **T·ªîNG QUAN LU·ªíNG HU·∫§N LUY·ªÜN**

T√†i li·ªáu n√†y m√¥ t·∫£ chi ti·∫øt t·ª´ng b∆∞·ªõc trong qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh QTransformer, t·ª´ khi b·∫Øt ƒë·∫ßu ch·∫°y script cho ƒë·∫øn khi t·∫°o ra k·∫øt qu·∫£ cu·ªëi c√πng.

### **ƒêi·ªÉm kh·ªüi ƒë·∫ßu**
```bash
# C√°ch kh·ªüi ch·∫°y hu·∫•n luy·ªán
./scripts/long_term_forecast/QCAAPatchTF_SupplyChain_Embedding.sh
```

---

## üìã **B∆Ø·ªöC 1: KH·ªûI T·∫†O V√Ä THI·∫æT L·∫¨P**

### **1.1 Entry Point - File `run.py`**
- **V·ªã tr√≠**: `/run.py` (th∆∞ m·ª•c g·ªëc)
- **Vai tr√≤**: Entry point ch√≠nh c·ªßa to√†n b·ªô h·ªá th·ªëng
- **Ch·ª©c nƒÉng ch√≠nh**:
  ```python
  # Thi·∫øt l·∫≠p seed cho reproducibility
  fix_seed = 2021
  random.seed(fix_seed)
  torch.manual_seed(fix_seed)
  np.random.seed(fix_seed)
  
  # Parsing arguments t·ª´ script
  parser = argparse.ArgumentParser()
  # ... ƒë·ªãnh nghƒ©a c√°c tham s·ªë
  ```

### **1.2 X√°c ƒë·ªãnh lo·∫°i Experiment**
```python
# Logic ch·ªçn experiment class (d√≤ng 150-154 trong run.py)
if args.task_name == 'long_term_forecast':
    if args.data in ['SupplyChainEmbedding', 'MultiRegionEmbedding'] or args.model in ['QCAAPatchTF_Embedding']:
        Exp = Exp_Long_Term_Forecast_Embedding  # ‚Üê Ch·ªçn class n√†y cho project
    else:
        Exp = Exp_Long_Term_Forecast
```

### **1.3 T·∫°o setting name**
```python
# T·∫°o t√™n unique cho experiment (d√≤ng 160-175)
setting = 'long_term_forecast_SupplyChain_Optimized_Phase1_v1_QCAAPatchTF_Embedding_...'
# Format: {task}_{model_id}_{model}_{data}_ft{features}_sl{seq_len}_...
```

---

## üìä **B∆Ø·ªöC 2: KH·ªûI T·∫†O EXPERIMENT CLASS**

### **2.1 Class `Exp_Long_Term_Forecast_Embedding`**
- **V·ªã tr√≠**: `/exp/exp_long_term_forecasting_embedding.py`
- **K·∫ø th·ª´a**: `Exp_Basic` t·ª´ `/exp/exp_basic.py`
- **Ch·ª©c nƒÉng kh·ªüi t·∫°o**:
  ```python
  def __init__(self, args):
      super().__init__(args)
      
      # Load target scaler cho denormalization
      self.target_scaler = None
      scaler_path = './scalers/target_scaler.pkl'
      if Path(scaler_path).exists():
          self.target_scaler = joblib.load(scaler_path)
  ```

### **2.2 Thi·∫øt l·∫≠p c√°c th√†nh ph·∫ßn ch√≠nh**
1. **Model**: G·ªçi `_build_model()` ‚Üí t·∫°o instance c·ªßa `QCAAPatchTF_Embedding`
2. **Optimizer**: G·ªçi `_select_optimizer()` ‚Üí Adam optimizer
3. **Loss function**: G·ªçi `_select_criterion()` ‚Üí `WeightedMSELoss` cho multi-market
4. **Data loaders**: G·ªçi `_get_data()` cho train/validation/test

---

## üèóÔ∏è **B∆Ø·ªöC 3: THI·∫æT L·∫¨P D·ªÆ LI·ªÜU**

### **3.1 Data Provider Factory**
- **V·ªã tr√≠**: `/data_provider/data_factory.py`
- **Ch·ª©c nƒÉng**: T·∫°o dataset v√† dataloader
```python
def data_provider(args, flag):
    # flag c√≥ th·ªÉ l√† 'train', 'val', ho·∫∑c 'test'
    Data = data_dict[args.data]  # SupplyChainEmbedding
    
    # T·∫°o dataset
    data_set = Data(args=args, root_path=args.root_path, ...)
    
    # T·∫°o dataloader v·ªõi custom collate function
    collate_fn = embedding_collate_fn  # X·ª≠ l√Ω categorical features
    data_loader = DataLoader(data_set, batch_size=args.batch_size, ...)
```

### **3.2 Dataset Loading**
- **V·ªã tr√≠**: `/data_provider/data_loader_embedding.py`
- **Class**: `Dataset_SupplyChain_Embedding`
- **Ch·ª©c nƒÉng**: 
  - Load file CSV t·ª´ `./dataset/supply_chain_optimized.csv`
  - Chu·∫©n b·ªã sequences v·ªõi length = 21 ng√†y
  - T·∫°o categorical features cho embedding
  - Normalization v·ªõi StandardScaler

### **3.3 C·∫•u tr√∫c d·ªØ li·ªáu tr·∫£ v·ªÅ**
```python
# M·ªói batch g·ªìm 5 th√†nh ph·∫ßn:
batch_x,           # [batch_size, seq_len, features] = [16, 21, 51]
batch_y,           # [batch_size, label_len+pred_len, c_out] = [16, 7, 3]  
batch_x_mark,      # [batch_size, seq_len, time_features]
batch_y_mark,      # [batch_size, label_len+pred_len, time_features]
categorical_features  # Dict v·ªõi c√°c embedding features
```

---

## üöÄ **B∆Ø·ªöC 4: QU√Å TR√åNH HU·∫§N LUY·ªÜN**

### **4.1 Main Training Loop**
- **V·ªã tr√≠**: `/exp/exp_long_term_forecasting_embedding.py` method `train()`
- **C·∫•u tr√∫c ch√≠nh**:
```python
def train(self, setting):
    # 1. L·∫•y data loaders
    train_data, train_loader = self._get_data(flag='train')
    vali_data, vali_loader = self._get_data(flag='val')
    test_data, test_loader = self._get_data(flag='test')
    
    # 2. Thi·∫øt l·∫≠p early stopping
    path = './checkpoints/' + setting
    early_stopping = EarlyStopping(patience=args.patience)
    
    # 3. Training loop ch√≠nh
    for epoch in range(args.train_epochs):
        # Training phase
        # Validation phase  
        # Early stopping check
```

### **4.2 Training Phase Chi Ti·∫øt**
```python
# M·ªói epoch g·ªìm c√°c b∆∞·ªõc:
for i, batch_data in enumerate(train_loader):
    # 1. Unpack d·ªØ li·ªáu
    batch_x, batch_y, batch_x_mark, batch_y_mark, categorical_features = batch_data
    
    # 2. Chuy·ªÉn l√™n GPU
    batch_x = batch_x.float().to(self.device)
    # ... c√°c tensor kh√°c
    
    # 3. Chu·∫©n b·ªã decoder input
    dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :])
    dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1)
    
    # 4. Forward pass
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, categorical_features)
    
    # 5. T√≠nh loss
    f_dim = -1 if self.args.features == 'MS' else 0
    outputs = outputs[:, -self.args.pred_len:, f_dim:]  # L·∫•y ph·∫ßn prediction
    batch_y = batch_y[:, -self.args.pred_len:, f_dim:]   # Ground truth
    loss = criterion(outputs, batch_y)  # WeightedMSELoss
    
    # 6. Backward pass
    loss.backward()
    model_optim.step()
```

### **4.3 Weighted Loss Function**
```python
class WeightedMSELoss(nn.Module):
    """Tr·ªçng s·ªë cho 3 th·ªã tr∆∞·ªùng: Europe=0.35, LATAM=0.30, USCA=0.35"""
    def forward(self, predictions, targets):
        # predictions: [batch, 7_days, 3_markets]
        # targets: [batch, 7_days, 3_markets]
        
        mse_per_market = torch.mean((predictions - targets) ** 2, dim=(0, 1))
        weights = torch.tensor([0.35, 0.30, 0.35])  # Market weights
        weighted_loss = torch.sum(weights * mse_per_market)
        return weighted_loss
```

---

## üìà **B∆Ø·ªöC 5: VALIDATION V√Ä EARLY STOPPING**

### **5.1 Validation Process**
```python
def vali(self, vali_data, vali_loader, criterion):
    total_loss = []
    self.model.eval()  # Chuy·ªÉn sang evaluation mode
    
    with torch.no_grad():  # Kh√¥ng t√≠nh gradient
        for batch_data in vali_loader:
            # T∆∞∆°ng t·ª± training nh∆∞ng kh√¥ng backward
            outputs = self.model(...)
            loss = criterion(outputs, batch_y)
            total_loss.append(loss.item())
    
    return np.average(total_loss)
```

### **5.2 Early Stopping Logic**
```python
# Sau m·ªói epoch:
vali_loss = self.vali(vali_data, vali_loader, criterion)
early_stopping(vali_loss, self.model, path)

if early_stopping.early_stop:
    print("Early stopping")
    break  # D·ª´ng training

# L∆∞u best model t·∫°i: ./checkpoints/{setting}/checkpoint.pth
```

---

## üîç **B∆Ø·ªöC 6: TESTING V√Ä ƒê√ÅNH GI√Å**

### **6.1 Testing Process**
- **Method**: `test()` trong `Exp_Long_Term_Forecast_Embedding`
- **Input**: Load best model t·ª´ checkpoint
- **Output**: Predictions v√† ground truth

```python
def test(self, setting, test=0):
    # 1. Load best model
    self.model.load_state_dict(torch.load(checkpoint_path))
    
    # 2. Run inference
    preds = []
    trues = []
    for batch_data in test_loader:
        outputs = self.model(...)
        preds.append(outputs.detach().cpu().numpy())
        trues.append(batch_y.detach().cpu().numpy())
    
    # 3. Concatenate results
    preds = np.concatenate(preds, axis=0)  # [70_samples, 7_days, 3_markets]
    trues = np.concatenate(trues, axis=0)
```

### **6.2 Metrics Calculation v·ªõi Denormalization**
```python
def _calculate_corrected_metrics(self, preds, trues):
    # 1. Load target scaler
    target_scaler = joblib.load('./scalers/target_scaler.pkl')
    
    # 2. Denormalize predictions v√† ground truth
    preds_denorm = target_scaler.inverse_transform(preds.reshape(-1, 1))
    trues_denorm = target_scaler.inverse_transform(trues.reshape(-1, 1))
    
    # 3. T√≠nh to√°n metrics th·ª±c t·∫ø
    mae = np.mean(np.abs(preds_denorm - trues_denorm))     # 13.75 orders
    mse = np.mean((preds_denorm - trues_denorm) ** 2)     # 262.82
    mape = np.mean(np.abs((trues_denorm - preds_denorm) / trues_denorm)) * 100  # 8.22%
```

---

## üìÅ **B∆Ø·ªöC 7: L∆ØU TR·ªÆ K·∫æT QU·∫¢**

### **7.1 C·∫•u tr√∫c th∆∞ m·ª•c output**
```
./test_results/long_term_forecast_SupplyChain_Optimized_Phase1_v1_QCAAPatchTF_Embedding_.../
‚îú‚îÄ‚îÄ metrics.npy              # Metrics g·ªëc [MAE, MSE, RMSE, MAPE, MSPE]
‚îú‚îÄ‚îÄ pred.npy                 # Predictions [70, 7, 3]
‚îú‚îÄ‚îÄ true.npy                 # Ground truth [70, 7, 3]
‚îú‚îÄ‚îÄ PHASE1_FINAL_SUMMARY.md  # B√°o c√°o t·ªïng k·∫øt
‚îú‚îÄ‚îÄ PHASE1_DETAILED_ANALYSIS_REPORT.md  # Ph√¢n t√≠ch chi ti·∫øt
‚îî‚îÄ‚îÄ phase1_daily_performance.csv  # Hi·ªáu su·∫•t theo ng√†y
```

### **7.2 Files checkpoint**
```
./checkpoints/long_term_forecast_SupplyChain_Optimized_Phase1_v1_.../
‚îî‚îÄ‚îÄ checkpoint.pth           # Best model weights
```

### **7.3 Logs training**
```
./logs/
‚îî‚îÄ‚îÄ training_{timestamp}.log # Chi ti·∫øt qu√° tr√¨nh training
```

---

## üîß **TH√ÄNH PH·∫¶N K·ª∏ THU·∫¨T CHI TI·∫æT**

### **Model Architecture Flow**
1. **Input Processing**: 
   - Continuous features: `[batch, 21, 51]` ‚Üí Embedding layers
   - Categorical features: Embedding tables ‚Üí concat v·ªõi continuous
   
2. **Transformer Encoder**:
   - MultiHead Attention v·ªõi `n_heads=8`
   - Feed Forward Network v·ªõi `d_ff=256`
   - Layer Normalization + Residual connections
   
3. **Decoder**:
   - Cross-attention v·ªõi encoder outputs
   - Masked self-attention cho prediction sequence
   
4. **Output Projection**:
   - Linear layer: `d_model=64` ‚Üí `c_out=3` (3 markets)
   - Shape: `[batch, 7, 3]` (7 ng√†y, 3 th·ªã tr∆∞·ªùng)

### **Memory v√† Performance**
- **GPU Memory**: ~2-4GB cho batch_size=16
- **Training Time**: ~3-5 ph√∫t cho 15 epochs
- **Convergence**: Early stopping th∆∞·ªùng x·∫£y ra ·ªü epoch 5-7

### **Key Parameters**
```python
seq_len=21      # Input sequence length (3 tu·∫ßn)
pred_len=7      # Prediction length (1 tu·∫ßn)  
enc_in=51       # Input features (ƒë√£ engineered)
c_out=3         # Output markets (Europe, LATAM, USCA)
d_model=64      # Model dimension
n_heads=8       # Attention heads
e_layers=3      # Encoder layers
batch_size=16   # Training batch size
```

---

## üéØ **T√ìNG T·∫ÆT LU·ªíNG HO√ÄN CH·ªàNH**

1. **Script kh·ªüi ch·∫°y** ‚Üí `run.py` 
2. **Parse arguments** ‚Üí T·∫°o `Exp_Long_Term_Forecast_Embedding`
3. **Load data** ‚Üí `data_factory.py` ‚Üí `Dataset_SupplyChain_Embedding`
4. **Build model** ‚Üí `QCAAPatchTF_Embedding` architecture
5. **Training loop** ‚Üí Forward/backward pass v·ªõi `WeightedMSELoss`
6. **Validation** ‚Üí Early stopping monitoring
7. **Testing** ‚Üí Best model inference
8. **Metrics** ‚Üí Denormalized evaluation
9. **Save results** ‚Üí Files trong `./test_results/`

### **Th·ªùi gian th·ª±c hi·ªán**
- **Setup**: 10-30 gi√¢y
- **Training**: 2-5 ph√∫t (5-15 epochs)
- **Testing**: 10-20 gi√¢y
- **Total**: ~3-6 ph√∫t

### **Command ƒë·ªÉ ch·∫°y**
```bash
# T·ª´ th∆∞ m·ª•c g·ªëc
./scripts/long_term_forecast/QCAAPatchTF_SupplyChain_Embedding.sh

# Ho·∫∑c tr·ª±c ti·∫øp
python run.py --task_name long_term_forecast --is_training 1 --model QCAAPatchTF_Embedding --data SupplyChainEmbedding [c√°c args kh√°c...]
```

---

*T√†i li·ªáu n√†y m√¥ t·∫£ chi ti·∫øt lu·ªìng training cho Phase 1. ƒê·ªÉ hi·ªÉu s√¢u h∆°n v·ªÅ t·ª´ng component, tham kh·∫£o code trong c√°c th∆∞ m·ª•c t∆∞∆°ng ·ª©ng.*
