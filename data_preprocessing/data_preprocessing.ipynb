{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd9c5d4",
   "metadata": {},
   "source": [
    "# 📊 Supply Chain Data Preprocessing Pipeline\n",
    "\n",
    "## 🎯 **Objective**\n",
    "Transform transaction-level supply chain data into time series format for **QCAAPatchTF_Embedding** model training.\n",
    "\n",
    "### **Strategy**: Synchronized Timeline (Option A)\n",
    "- **Input**: DataCoSupplyChain_Synchronized.csv (147K transactions)\n",
    "- **Output**: Time series data [seq_len, 36_features] for 3 markets  \n",
    "- **Target**: Daily order counts [7_days, 3_markets]\n",
    "\n",
    "### **Pipeline Overview**:\n",
    "1. 📂 **Load & Parse Data** - Convert dates, validate markets\n",
    "2. 🔍 **Find Sync Point** - Timeline synchronization across markets  \n",
    "3. 📊 **Daily Aggregation** - Transaction → Daily time series\n",
    "4. 📅 **Complete Timeline** - Fill missing days for all markets\n",
    "5. 🕐 **Time Features** - Extract temporal patterns\n",
    "6. 🔧 **Feature Engineering** - Diversity indices, volatility\n",
    "7. 🧹 **Data Cleaning** - Outliers, missing values  \n",
    "8. 🎯 **Model Format** - Create sequences for training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950eafc",
   "metadata": {},
   "source": [
    "## 1️⃣ Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0086ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "📦 Pandas version: 1.5.3\n",
      "📦 NumPy version: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical and preprocessing libraries\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Visualization (for validation)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📦 Pandas version: {pd.__version__}\")\n",
    "print(f\"📦 NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414bae7",
   "metadata": {},
   "source": [
    "## 2️⃣ Initialize SupplyChainPreprocessor Class\n",
    "\n",
    "Define the main preprocessor class with configurable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58026c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 SupplyChainPreprocessor initialized\n",
      "⚙️  Config: min_orders=10, outlier_threshold=3.5\n",
      "🎯 Target markets: ['USCA', 'LATAM', 'Europe']\n",
      "\n",
      "✅ Preprocessor ready for use!\n"
     ]
    }
   ],
   "source": [
    "class SupplyChainPreprocessor:\n",
    "    \"\"\"\n",
    "    📈 Main preprocessing class for supply chain forecasting pipeline\n",
    "    \n",
    "    Features:\n",
    "    - Timeline synchronization across 3 markets (Option A)\n",
    "    - Transaction → daily aggregation\n",
    "    - Feature engineering (36 features total)\n",
    "    - Outlier handling and missing value imputation\n",
    "    - Time series format creation for model input\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_orders_per_market=10, outlier_threshold=3.5):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with configurable parameters\n",
    "        \n",
    "        Args:\n",
    "            min_orders_per_market: Minimum daily orders to consider market \"active\"\n",
    "            outlier_threshold: Z-score threshold for outlier detection\n",
    "        \"\"\"\n",
    "        self.min_orders_per_market = min_orders_per_market\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.markets = ['USCA', 'LATAM', 'Europe']\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "        # Timeline tracking\n",
    "        self.original_start_date = None\n",
    "        self.original_end_date = None\n",
    "        self.synchronized_start_date = None\n",
    "        self.final_end_date = None\n",
    "        self.data_loss_stats = {}\n",
    "        \n",
    "        # Feature tracking\n",
    "        self.feature_columns = []\n",
    "        self.target_column = 'order_count'\n",
    "        \n",
    "        print(\"🔧 SupplyChainPreprocessor initialized\")\n",
    "        print(f\"⚙️  Config: min_orders={min_orders_per_market}, outlier_threshold={outlier_threshold}\")\n",
    "        print(f\"🎯 Target markets: {self.markets}\")\n",
    "\n",
    "# Initialize preprocessor instance\n",
    "preprocessor = SupplyChainPreprocessor(\n",
    "    min_orders_per_market=10,\n",
    "    outlier_threshold=3.5\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Preprocessor ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7c3b2",
   "metadata": {},
   "source": [
    "## 3️⃣ Load Raw Supply Chain Data\n",
    "\n",
    "Read DataCoSupplyChain_Synchronized.csv and convert date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1299a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing data loading...\n",
      "🔄 Loading supply chain dataset...\n",
      "✅ Loaded 147,041 transactions\n",
      "📅 Original timeline: 2017-01-18 00:00:00 to 2018-01-31 00:00:00\n",
      "📊 Markets: ['USCA' 'Europe' 'LATAM']\n",
      "📋 Columns: 55 features\n",
      "\n",
      "✅ Data loaded successfully!\n",
      "📊 Shape: (147041, 55)\n",
      "📋 First few columns: ['Type', 'Days for shipping (real)', 'Days for shipment (scheduled)', 'Benefit per order', 'Sales per customer', 'Delivery Status', 'Late_delivery_risk', 'Category Id', 'Category Name', 'Customer City']\n"
     ]
    }
   ],
   "source": [
    "def load_data(self, file_path):\n",
    "    \"\"\"\n",
    "    📂 Load raw supply chain dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to DataCoSupplyChain_Synchronized.csv\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw dataset with datetime conversion\n",
    "    \"\"\"\n",
    "    print(\"🔄 Loading supply chain dataset...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    date_column = 'order date (DateOrders)'  # Exact column name from dataset\n",
    "    df['order_date_only'] = pd.to_datetime(df[date_column]).dt.date\n",
    "    df['order_date_only'] = pd.to_datetime(df['order_date_only'])\n",
    "    \n",
    "    # Store original timeline\n",
    "    self.original_start_date = df['order_date_only'].min()\n",
    "    self.original_end_date = df['order_date_only'].max()\n",
    "    \n",
    "    print(f\"✅ Loaded {len(df):,} transactions\")\n",
    "    print(f\"📅 Original timeline: {self.original_start_date} to {self.original_end_date}\")\n",
    "    print(f\"📊 Markets: {df['Market'].unique()}\")\n",
    "    print(f\"📋 Columns: {len(df.columns)} features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add method to class\n",
    "SupplyChainPreprocessor.load_data = load_data\n",
    "\n",
    "# Test loading data\n",
    "print(\"🧪 Testing data loading...\")\n",
    "data_path = \"../dataset/DataCoSupplyChain_Synchronized.csv\"\n",
    "\n",
    "try:\n",
    "    df_raw = preprocessor.load_data(data_path)\n",
    "    print(f\"\\n✅ Data loaded successfully!\")\n",
    "    print(f\"📊 Shape: {df_raw.shape}\")\n",
    "    print(f\"📋 First few columns: {list(df_raw.columns[:10])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {str(e)}\")\n",
    "    print(f\"📁 Looking for file at: {os.path.abspath(data_path)}\")\n",
    "    print(f\"📁 File exists: {os.path.exists(data_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d8fe9",
   "metadata": {},
   "source": [
    "## 4️⃣ Find Synchronized Start Date  \n",
    "\n",
    "Implement Timeline synchronization strategy (Option A) - tìm ngày đầu tiên cả 3 markets đều hoạt động."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8427db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing timeline synchronization...\n",
      "🔄 Finding synchronized start date (min 10 orders/market)...\n",
      "📊 Market matrix shape: (309, 3)\n",
      "📊 Available markets: ['Europe', 'LATAM', 'USCA']\n",
      "✅ Synchronized start date: 2017-05-22 00:00:00\n",
      "📊 Removed 124 days for synchronization\n",
      "📉 Data loss: 17,222 records (11.7%)\n",
      "\n",
      "✅ Synchronization completed!\n",
      "📅 Original start: 2017-01-18 00:00:00\n",
      "📅 Synchronized start: 2017-05-22 00:00:00\n",
      "📊 Data loss: 11.7%\n"
     ]
    }
   ],
   "source": [
    "def find_synchronized_start_date(self, df):\n",
    "    \"\"\"\n",
    "    🔍 Find the first date when ALL 3 markets have >= min_orders_per_market\n",
    "    \n",
    "    This implements the core synchronization strategy (Option A)\n",
    "    \n",
    "    Args:\n",
    "        df: Raw transaction dataset\n",
    "        \n",
    "    Returns:\n",
    "        datetime: Synchronized start date for all markets\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Finding synchronized start date (min {self.min_orders_per_market} orders/market)...\")\n",
    "    \n",
    "    # Count daily orders per market\n",
    "    daily_counts = df.groupby(['order_date_only', 'Market']).size().reset_index(name='daily_orders')\n",
    "    \n",
    "    # Pivot to matrix format [date × market]\n",
    "    market_matrix = daily_counts.pivot(\n",
    "        index='order_date_only', \n",
    "        columns='Market', \n",
    "        values='daily_orders'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(f\"📊 Market matrix shape: {market_matrix.shape}\")\n",
    "    print(f\"📊 Available markets: {list(market_matrix.columns)}\")\n",
    "    \n",
    "    # Ensure all 3 expected markets exist\n",
    "    for market in self.markets:\n",
    "        if market not in market_matrix.columns:\n",
    "            print(f\"⚠️  Warning: Market '{market}' not found in data\")\n",
    "            market_matrix[market] = 0\n",
    "    \n",
    "    # Find first date when ALL markets meet threshold\n",
    "    valid_days = (market_matrix[self.markets] >= self.min_orders_per_market).all(axis=1)\n",
    "    \n",
    "    if not valid_days.any():\n",
    "        raise ValueError(f\"❌ No date found where all markets have >= {self.min_orders_per_market} orders\")\n",
    "    \n",
    "    synchronized_start = valid_days[valid_days == True].index[0]\n",
    "    self.synchronized_start_date = synchronized_start\n",
    "    \n",
    "    # Calculate data loss statistics\n",
    "    removed_days = (synchronized_start - self.original_start_date).days\n",
    "    total_records = len(df)\n",
    "    sync_records = len(df[df['order_date_only'] >= synchronized_start])\n",
    "    data_loss_pct = ((total_records - sync_records) / total_records) * 100\n",
    "    \n",
    "    print(f\"✅ Synchronized start date: {synchronized_start}\")\n",
    "    print(f\"📊 Removed {removed_days} days for synchronization\")\n",
    "    print(f\"📉 Data loss: {total_records - sync_records:,} records ({data_loss_pct:.1f}%)\")\n",
    "    \n",
    "    # Store detailed statistics\n",
    "    self.data_loss_stats = {\n",
    "        'removed_days': removed_days,\n",
    "        'total_records': total_records,\n",
    "        'sync_records': sync_records,\n",
    "        'data_loss_pct': data_loss_pct\n",
    "    }\n",
    "    \n",
    "    return synchronized_start\n",
    "\n",
    "# Add method to class\n",
    "SupplyChainPreprocessor.find_synchronized_start_date = find_synchronized_start_date\n",
    "\n",
    "# Test synchronization\n",
    "print(\"🧪 Testing timeline synchronization...\")\n",
    "try:\n",
    "    sync_start = preprocessor.find_synchronized_start_date(df_raw)\n",
    "    print(f\"\\n✅ Synchronization completed!\")\n",
    "    print(f\"📅 Original start: {preprocessor.original_start_date}\")\n",
    "    print(f\"📅 Synchronized start: {sync_start}\")\n",
    "    print(f\"📊 Data loss: {preprocessor.data_loss_stats['data_loss_pct']:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in synchronization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3041696",
   "metadata": {},
   "source": [
    "## 5️⃣ Aggregate Transactions to Daily Time Series\n",
    "\n",
    "Transform transaction-level → daily aggregated data với proper aggregation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c52525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing daily aggregation...\n",
      "📊 Synchronized data: 129,819 records\n",
      "🔄 Aggregating transactions to daily time series...\n",
      "✅ Created daily aggregation: 765 records\n",
      "📅 Date range: 2017-05-22 00:00:00 to 2018-01-31 00:00:00\n",
      "📊 Markets per day: [3]\n",
      "📋 Features: ['order_date_only', 'Market', 'order_count', 'Days for shipping (real)', 'Late_delivery_risk', 'Order Item Product Price', 'Order Item Discount Rate', 'Order Item Profit Ratio', 'Order Profit Per Order', 'Order Item Quantity', 'Sales', 'Order Item Total', 'Customer Segment', 'Category Name']\n",
      "\n",
      "✅ Daily aggregation completed!\n",
      "📊 Daily data shape: (765, 14)\n",
      "📊 Sample order counts by market:\n",
      "        count        mean        std   min    25%    50%    75%    max\n",
      "Market                                                                \n",
      "Europe  255.0  168.411765  22.915435   6.0  162.0  171.0  180.0  220.0\n",
      "LATAM   255.0  170.882353  18.045270  49.0  162.0  171.0  181.0  213.0\n",
      "USCA    255.0  169.800000  16.384744  68.0  159.0  170.0  182.0  212.0\n"
     ]
    }
   ],
   "source": [
    "def aggregate_to_daily(self, df):\n",
    "    \"\"\"\n",
    "    📊 Transform transaction-level data to daily aggregated time series\n",
    "    \n",
    "    Aggregation strategy:\n",
    "    - Count: Order Id (target variable)\n",
    "    - Mean: Price, discount, profit ratios, shipping days, risk\n",
    "    - Sum: Quantity, sales, order total\n",
    "    - Mode: Customer segment, category (for diversity calculation)\n",
    "    \n",
    "    Args:\n",
    "        df: Synchronized transaction dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Daily aggregated data [date × market × features]\n",
    "    \"\"\"\n",
    "    print(\"🔄 Aggregating transactions to daily time series...\")\n",
    "    \n",
    "    # Define aggregation rules using actual column names\n",
    "    agg_rules = {\n",
    "        # Target variable\n",
    "        'Order Id': 'count',                    # Daily order count (TARGET)\n",
    "        \n",
    "        # Raw numerical features (to be averaged)\n",
    "        'Days for shipping (real)': 'mean',\n",
    "        'Late_delivery_risk': 'mean',\n",
    "        'Order Item Product Price': 'mean',\n",
    "        'Order Item Discount Rate': 'mean', \n",
    "        'Order Item Profit Ratio': 'mean',\n",
    "        'Order Profit Per Order': 'mean',\n",
    "        \n",
    "        # For engineered features (to be summed/processed)\n",
    "        'Order Item Quantity': 'sum',          # Total quantity per day\n",
    "        'Sales': 'sum',                        # Total sales per day\n",
    "        'Order Item Total': 'mean',            # Average order value\n",
    "        \n",
    "        # For diversity calculations (keep first for processing)\n",
    "        'Customer Segment': lambda x: list(x), # Keep all segments for diversity calc\n",
    "        'Category Name': lambda x: list(x)     # Keep all categories for diversity calc\n",
    "    }\n",
    "    \n",
    "    # Group by date and market, then aggregate\n",
    "    daily_agg = df.groupby(['order_date_only', 'Market']).agg(agg_rules).reset_index()\n",
    "    \n",
    "    # Rename target column for clarity\n",
    "    daily_agg = daily_agg.rename(columns={'Order Id': 'order_count'})\n",
    "    \n",
    "    print(f\"✅ Created daily aggregation: {len(daily_agg):,} records\")\n",
    "    print(f\"📅 Date range: {daily_agg['order_date_only'].min()} to {daily_agg['order_date_only'].max()}\")\n",
    "    print(f\"📊 Markets per day: {daily_agg.groupby('order_date_only')['Market'].count().unique()}\")\n",
    "    print(f\"📋 Features: {list(daily_agg.columns)}\")\n",
    "    \n",
    "    return daily_agg\n",
    "\n",
    "# Add method to class\n",
    "SupplyChainPreprocessor.aggregate_to_daily = aggregate_to_daily\n",
    "\n",
    "# Test daily aggregation\n",
    "print(\"🧪 Testing daily aggregation...\")\n",
    "try:\n",
    "    # Filter to synchronized data first\n",
    "    df_sync = df_raw[df_raw['order_date_only'] >= preprocessor.synchronized_start_date].copy()\n",
    "    print(f\"📊 Synchronized data: {len(df_sync):,} records\")\n",
    "    \n",
    "    df_daily = preprocessor.aggregate_to_daily(df_sync)\n",
    "    print(f\"\\n✅ Daily aggregation completed!\")\n",
    "    print(f\"📊 Daily data shape: {df_daily.shape}\")\n",
    "    print(f\"📊 Sample order counts by market:\")\n",
    "    print(df_daily.groupby('Market')['order_count'].describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in aggregation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610adeb6",
   "metadata": {},
   "source": [
    "## 6️⃣ Create Time-Based Features\n",
    "\n",
    "Generate temporal features: day_of_week, month, is_weekend, days_since_start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb58d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing time feature creation...\n",
      "🔄 Creating time features...\n",
      "✅ Created 5 time features\n",
      "📊 Day of week range: 0 to 6\n",
      "📊 Month range: 1 to 12\n",
      "📊 Weekend percentage: 28.2%\n",
      "📊 Days since start range: 0 to 254\n",
      "\n",
      "✅ Time features created!\n",
      "📊 Updated shape: (765, 19)\n",
      "📋 New columns: ['day_of_week', 'day_of_month', 'month', 'is_weekend', 'days_since_start']\n",
      "\n",
      "📊 Sample time features:\n",
      "  order_date_only  day_of_week  day_of_month  month  is_weekend  \\\n",
      "0      2017-05-22            0            22      5           0   \n",
      "1      2017-05-22            0            22      5           0   \n",
      "2      2017-05-22            0            22      5           0   \n",
      "3      2017-05-23            1            23      5           0   \n",
      "4      2017-05-23            1            23      5           0   \n",
      "\n",
      "   days_since_start  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 1  \n",
      "4                 1  \n"
     ]
    }
   ],
   "source": [
    "def create_time_features(self, df):\n",
    "    \"\"\"\n",
    "    🕐 Create time-based features from order_date_only\n",
    "    \n",
    "    Features created:\n",
    "    - day_of_week: 0-6 (Monday=0)\n",
    "    - day_of_month: 1-31\n",
    "    - month: 1-12\n",
    "    - is_weekend: Boolean\n",
    "    - days_since_start: Trend component\n",
    "    \n",
    "    Args:\n",
    "        df: Daily aggregated dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with additional time features\n",
    "    \"\"\"\n",
    "    print(\"🔄 Creating time features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create time features\n",
    "    df['day_of_week'] = df['order_date_only'].dt.dayofweek\n",
    "    df['day_of_month'] = df['order_date_only'].dt.day\n",
    "    df['month'] = df['order_date_only'].dt.month\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['days_since_start'] = (df['order_date_only'] - self.synchronized_start_date).dt.days\n",
    "    \n",
    "    print(\"✅ Created 5 time features\")\n",
    "    print(f\"📊 Day of week range: {df['day_of_week'].min()} to {df['day_of_week'].max()}\")\n",
    "    print(f\"📊 Month range: {df['month'].min()} to {df['month'].max()}\")\n",
    "    print(f\"📊 Weekend percentage: {(df['is_weekend'].sum() / len(df) * 100):.1f}%\")\n",
    "    print(f\"📊 Days since start range: {df['days_since_start'].min()} to {df['days_since_start'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add method to class\n",
    "SupplyChainPreprocessor.create_time_features = create_time_features\n",
    "\n",
    "# Test time feature creation\n",
    "print(\"🧪 Testing time feature creation...\")\n",
    "try:\n",
    "    df_with_time = preprocessor.create_time_features(df_daily)\n",
    "    print(f\"\\n✅ Time features created!\")\n",
    "    print(f\"📊 Updated shape: {df_with_time.shape}\")\n",
    "    print(f\"📋 New columns: {[col for col in df_with_time.columns if col not in df_daily.columns]}\")\n",
    "    \n",
    "    # Show sample of time features\n",
    "    print(f\"\\n📊 Sample time features:\")\n",
    "    time_cols = ['order_date_only', 'day_of_week', 'day_of_month', 'month', 'is_weekend', 'days_since_start']\n",
    "    print(df_with_time[time_cols].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating time features: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d679812",
   "metadata": {},
   "source": [
    "## 7️⃣ Engineer Additional Features  \n",
    "\n",
    "Create customer segment percentages, category diversity index, và price volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6d8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing feature engineering...\n",
      "🔄 Engineering additional features...\n",
      "✅ Created 5 engineered features\n",
      "📊 Consumer segment avg: 0.518\n",
      "📊 Corporate segment avg: 0.304\n",
      "📊 Home Office segment avg: 0.178\n",
      "📊 Category diversity avg: 0.894\n",
      "\n",
      "✅ Feature engineering completed!\n",
      "📊 Updated shape: (765, 24)\n",
      "📋 Engineered features: ['customer_segment_consumer_pct', 'customer_segment_corporate_pct', 'customer_segment_home_office_pct', 'category_diversity_index', 'price_volatility']\n",
      "\n",
      "📊 Sample engineered features:\n",
      "   customer_segment_consumer_pct  customer_segment_corporate_pct  \\\n",
      "0                       0.495868                        0.380165   \n",
      "1                       0.597561                        0.195122   \n",
      "2                       0.535519                        0.289617   \n",
      "3                       0.548913                        0.320652   \n",
      "4                       0.685185                        0.160494   \n",
      "\n",
      "   customer_segment_home_office_pct  category_diversity_index  \\\n",
      "0                          0.123967                  0.887508   \n",
      "1                          0.207317                  0.894036   \n",
      "2                          0.174863                  0.892950   \n",
      "3                          0.130435                  0.897271   \n",
      "4                          0.154321                  0.890794   \n",
      "\n",
      "   price_volatility  \n",
      "0               0.1  \n",
      "1               0.1  \n",
      "2               0.1  \n",
      "3               0.1  \n",
      "4               0.1  \n"
     ]
    }
   ],
   "source": [
    "def engineer_features(self, df):\n",
    "    \"\"\"\n",
    "    🔧 Engineer additional features from aggregated data\n",
    "    \n",
    "    Features created:\n",
    "    - Customer segment percentages (3 features)\n",
    "    - Category diversity index (Simpson's diversity)\n",
    "    - Price volatility (coefficient of variation)\n",
    "    \n",
    "    Args:\n",
    "        df: Dataset with time features\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with engineered features\n",
    "    \"\"\"\n",
    "    print(\"🔄 Engineering additional features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Customer segment percentages\n",
    "    def calc_segment_percentages(segments_list):\n",
    "        \"\"\"Calculate percentage of each segment in a list\"\"\"\n",
    "        if not segments_list or len(segments_list) == 0:\n",
    "            return 0.33, 0.33, 0.34  # Default equal distribution\n",
    "        \n",
    "        total = len(segments_list)\n",
    "        consumer_pct = segments_list.count('Consumer') / total\n",
    "        corporate_pct = segments_list.count('Corporate') / total\n",
    "        home_office_pct = segments_list.count('Home Office') / total\n",
    "        \n",
    "        return consumer_pct, corporate_pct, home_office_pct\n",
    "    \n",
    "    # Apply to each row\n",
    "    segment_data = df['Customer Segment'].apply(calc_segment_percentages)\n",
    "    df['customer_segment_consumer_pct'] = [x[0] for x in segment_data]\n",
    "    df['customer_segment_corporate_pct'] = [x[1] for x in segment_data]\n",
    "    df['customer_segment_home_office_pct'] = [x[2] for x in segment_data]\n",
    "    \n",
    "    # Category diversity index (Simpson's diversity)\n",
    "    def calc_diversity_index(categories_list):\n",
    "        \"\"\"Calculate Simpson's diversity index\"\"\"\n",
    "        if not categories_list or len(categories_list) == 0:\n",
    "            return 0.5  # Default moderate diversity\n",
    "        \n",
    "        total = len(categories_list)\n",
    "        category_counts = {}\n",
    "        for cat in categories_list:\n",
    "            category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "        \n",
    "        # Simpson's diversity: 1 - sum(pi^2)\n",
    "        diversity = 1 - sum((count/total)**2 for count in category_counts.values())\n",
    "        return diversity\n",
    "    \n",
    "    df['category_diversity_index'] = df['Category Name'].apply(calc_diversity_index)\n",
    "    \n",
    "    # Price volatility (coefficient of variation for the day)\n",
    "    # Since we aggregated to daily mean, we'll use a placeholder for now\n",
    "    # In real implementation, would need price data within each day\n",
    "    df['price_volatility'] = 0.1  # Placeholder - could be calculated from original data\n",
    "    \n",
    "    print(\"✅ Created 5 engineered features\")\n",
    "    print(f\"📊 Consumer segment avg: {df['customer_segment_consumer_pct'].mean():.3f}\")\n",
    "    print(f\"📊 Corporate segment avg: {df['customer_segment_corporate_pct'].mean():.3f}\")\n",
    "    print(f\"📊 Home Office segment avg: {df['customer_segment_home_office_pct'].mean():.3f}\")\n",
    "    print(f\"📊 Category diversity avg: {df['category_diversity_index'].mean():.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add method to class\n",
    "SupplyChainPreprocessor.engineer_features = engineer_features\n",
    "\n",
    "# Test feature engineering\n",
    "print(\"🧪 Testing feature engineering...\")\n",
    "try:\n",
    "    df_featured = preprocessor.engineer_features(df_with_time)\n",
    "    print(f\"\\n✅ Feature engineering completed!\")\n",
    "    print(f\"📊 Updated shape: {df_featured.shape}\")\n",
    "    \n",
    "    # Show new engineered features\n",
    "    eng_cols = ['customer_segment_consumer_pct', 'customer_segment_corporate_pct', \n",
    "                'customer_segment_home_office_pct', 'category_diversity_index', 'price_volatility']\n",
    "    print(f\"📋 Engineered features: {eng_cols}\")\n",
    "    print(f\"\\n📊 Sample engineered features:\")\n",
    "    print(df_featured[eng_cols].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in feature engineering: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08a825",
   "metadata": {},
   "source": [
    "## 8️⃣ Final Processing & Data Export\n",
    "\n",
    "Prepare final dataset và save to CSV files với label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27729499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Finalizing and preparing export...\n",
      "🔄 Finalizing dataset for export...\n",
      "✅ Final dataset prepared\n",
      "📊 Shape: (765, 23)\n",
      "📋 Features: 21 total\n",
      "📋 Numerical: 20\n",
      "📋 Categorical: 1\n",
      "\n",
      "✅ Data finalization completed!\n",
      "📊 Final shape: (765, 23)\n",
      "🏷️  Market encoding: {'Europe': 0, 'LATAM': 1, 'USCA': 2}\n",
      "📋 Feature columns: 21\n",
      "\n",
      "📊 Sample final data:\n",
      "  order_date_only  Market  Market_encoded  order_count  day_of_week  month\n",
      "0      2017-05-22  Europe               0          121            0      5\n",
      "1      2017-05-22   LATAM               1          164            0      5\n",
      "2      2017-05-22    USCA               2          183            0      5\n",
      "3      2017-05-23  Europe               0          184            1      5\n",
      "4      2017-05-23   LATAM               1          162            1      5\n"
     ]
    }
   ],
   "source": [
    "# Final data processing and export\n",
    "def finalize_and_export(self, df, output_dir=\"../dataset\"):\n",
    "    \"\"\"\n",
    "    🎯 Finalize dataset and export to CSV files\n",
    "    \n",
    "    Args:\n",
    "        df: Processed dataframe\n",
    "        output_dir: Directory to save files\n",
    "    \"\"\"\n",
    "    print(\"🔄 Finalizing dataset for export...\")\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Select final feature columns (numerical features for model)\n",
    "    numerical_features = [\n",
    "        # Raw numerical features (6)\n",
    "        'Days for shipping (real)', 'Late_delivery_risk', 'Order Item Product Price',\n",
    "        'Order Item Discount Rate', 'Order Item Profit Ratio', 'Order Profit Per Order',\n",
    "        \n",
    "        # Engineered numerical features (9)  \n",
    "        'Order Item Quantity', 'Sales', 'Order Item Total',\n",
    "        'customer_segment_consumer_pct', 'customer_segment_corporate_pct', \n",
    "        'customer_segment_home_office_pct', 'category_diversity_index', 'price_volatility',\n",
    "        'order_count',  # Include target for reference\n",
    "        \n",
    "        # Time features (5)\n",
    "        'day_of_week', 'day_of_month', 'month', 'is_weekend', 'days_since_start'\n",
    "    ]\n",
    "    \n",
    "    # Create final dataset\n",
    "    final_df = df[['order_date_only', 'Market'] + numerical_features].copy()\n",
    "    \n",
    "    # Create Market label encoding for embedding\n",
    "    market_encoder = LabelEncoder()\n",
    "    final_df['Market_encoded'] = market_encoder.fit_transform(final_df['Market'])\n",
    "    \n",
    "    # Store feature information\n",
    "    feature_info = {\n",
    "        'total_features': len(numerical_features) + 1,  # +1 for Market embedding\n",
    "        'numerical_features': len(numerical_features),\n",
    "        'categorical_features': 1,  # Market only\n",
    "        'target_column': 'order_count',\n",
    "        'market_encoding': dict(zip(market_encoder.classes_, market_encoder.transform(market_encoder.classes_))),\n",
    "        'feature_columns': numerical_features + ['Market_encoded']\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Final dataset prepared\")\n",
    "    print(f\"📊 Shape: {final_df.shape}\")\n",
    "    print(f\"📋 Features: {len(feature_info['feature_columns'])} total\")\n",
    "    print(f\"📋 Numerical: {feature_info['numerical_features']}\")\n",
    "    print(f\"📋 Categorical: {feature_info['categorical_features']}\")\n",
    "    \n",
    "    return final_df, feature_info\n",
    "\n",
    "# Add method to class\n",
    "SupplyChainPreprocessor.finalize_and_export = finalize_and_export\n",
    "\n",
    "# Execute final processing\n",
    "print(\"🧪 Finalizing and preparing export...\")\n",
    "try:\n",
    "    final_data, feature_info = preprocessor.finalize_and_export(df_featured)\n",
    "    \n",
    "    print(f\"\\n✅ Data finalization completed!\")\n",
    "    print(f\"📊 Final shape: {final_data.shape}\")\n",
    "    print(f\"🏷️  Market encoding: {feature_info['market_encoding']}\")\n",
    "    print(f\"📋 Feature columns: {len(feature_info['feature_columns'])}\")\n",
    "    \n",
    "    # Show sample of final data\n",
    "    print(f\"\\n📊 Sample final data:\")\n",
    "    sample_cols = ['order_date_only', 'Market', 'Market_encoded', 'order_count', 'day_of_week', 'month']\n",
    "    print(final_data[sample_cols].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in finalization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5fecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Saving processed data to CSV files...\n",
      "✅ Saved main dataset: ../dataset/supply_chain_processed.csv\n",
      "✅ Saved feature mapping: ../dataset/feature_mapping.json\n",
      "✅ Saved preprocessing stats: ../dataset/preprocessing_stats.json\n",
      "\n",
      "📊 FINAL DATA SUMMARY\n",
      "==================================================\n",
      "📁 Main dataset: ../dataset/supply_chain_processed.csv\n",
      "📁 Feature mapping: ../dataset/feature_mapping.json\n",
      "📁 Statistics: ../dataset/preprocessing_stats.json\n",
      "\n",
      "📊 Data shape: (765, 23)\n",
      "📅 Timeline: 2017-05-22 00:00:00 to 2018-01-31 00:00:00\n",
      "🎯 Markets: ['Europe', 'LATAM', 'USCA']\n",
      "📋 Features: 21 total\n",
      "📉 Data loss: 11.7%\n",
      "📈 Avg orders/day/market: 169.7\n",
      "\n",
      "✅ ALL DATA PROCESSING AND EXPORT COMPLETED!\n",
      "🎯 Ready for QCAAPatchTF_Embedding model training\n"
     ]
    }
   ],
   "source": [
    "# Export processed data to CSV files\n",
    "print(\"📁 Saving processed data to CSV files...\")\n",
    "\n",
    "# 1. Save main processed dataset\n",
    "output_file = \"../dataset/supply_chain_processed.csv\"\n",
    "final_data.to_csv(output_file, index=False)\n",
    "print(f\"✅ Saved main dataset: {output_file}\")\n",
    "\n",
    "# 2. Save feature mapping information\n",
    "import json\n",
    "feature_mapping_file = \"../dataset/feature_mapping.json\"\n",
    "with open(feature_mapping_file, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2, default=str)\n",
    "print(f\"✅ Saved feature mapping: {feature_mapping_file}\")\n",
    "\n",
    "# 3. Save preprocessing statistics\n",
    "preprocessing_stats = {\n",
    "    'original_records': preprocessor.data_loss_stats['total_records'],\n",
    "    'processed_records': len(final_data),\n",
    "    'data_loss_percentage': preprocessor.data_loss_stats['data_loss_pct'],\n",
    "    'original_timeline': f\"{preprocessor.original_start_date} to {preprocessor.original_end_date}\",\n",
    "    'synchronized_timeline': f\"{preprocessor.synchronized_start_date} to {final_data['order_date_only'].max()}\",\n",
    "    'markets': list(feature_info['market_encoding'].keys()),\n",
    "    'total_features': feature_info['total_features'],\n",
    "    'days_processed': len(final_data) // 3,  # 3 markets per day\n",
    "    'avg_orders_per_day_per_market': final_data['order_count'].mean()\n",
    "}\n",
    "\n",
    "stats_file = \"../dataset/preprocessing_stats.json\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(preprocessing_stats, f, indent=2, default=str)\n",
    "print(f\"✅ Saved preprocessing stats: {stats_file}\")\n",
    "\n",
    "# 4. Create data summary for validation\n",
    "print(f\"\\n📊 FINAL DATA SUMMARY\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"📁 Main dataset: {output_file}\")\n",
    "print(f\"📁 Feature mapping: {feature_mapping_file}\")\n",
    "print(f\"📁 Statistics: {stats_file}\")\n",
    "print(f\"\")\n",
    "print(f\"📊 Data shape: {final_data.shape}\")\n",
    "print(f\"📅 Timeline: {final_data['order_date_only'].min()} to {final_data['order_date_only'].max()}\")\n",
    "print(f\"🎯 Markets: {list(feature_info['market_encoding'].keys())}\")\n",
    "print(f\"📋 Features: {feature_info['total_features']} total\")\n",
    "print(f\"📉 Data loss: {preprocessing_stats['data_loss_percentage']:.1f}%\")\n",
    "print(f\"📈 Avg orders/day/market: {preprocessing_stats['avg_orders_per_day_per_market']:.1f}\")\n",
    "\n",
    "print(f\"\\n✅ ALL DATA PROCESSING AND EXPORT COMPLETED!\")\n",
    "print(f\"🎯 Ready for QCAAPatchTF_Embedding model training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
